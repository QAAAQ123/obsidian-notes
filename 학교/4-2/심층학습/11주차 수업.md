## RNN(순환 신경망)
- 순차 데이터 형태를 처리하기 위해서 고안된 신경망
- 순차데이터
	- 순서,시간 의존성을 가진 데이터(시계열 데이터)
	- 순서 고려해야함(순서 바뀌면 의미가 달라짐)
- MLP,CNN는 순차 데이터 처리 불가능: 순서가 없는 고정된 입력만 처리 가능
	- 순서적,시간적 의존성 반영 못함
	- 다른 부분과 연결하여 학습하지 못함
- 하나의 입력으로 여러개의 시점에서 여러개의 출력값(one-to-many)
- many-to-one: 분류 작업(긍정or부정으로 나눌때)
- many-to-one: 번역
- RNN-순차적,시간적 의존성 학습
	- x_1 -> y_1 -> x_2 -> 이전 학습(x1->y1)정보 고려 ->y2->x3->이전학습 정보 고려 -> ...
- 은닉상태를 통해 모델 내부의 파라미터 통해서 내재화(인코딩)하여 다음 단계로 전달(h_t)
- h_t=f_w(이전 히든,현재 입력)
- y_t = f(h_t) -이전 + 현재
- W_hh: h차원 ->h차원
- W_xh: x차원 ->h차원
- W_ny: 은닉층 벡터 형태를 출력 형태로 변형
- ReLu사용하는 이유: 음수가 들어오면 그 이후값 반영 안하기 때문에
- 하이퍼볼릭탄젠드: 이전값 계속 반영하기 때문에
- y_t=y_ny x h_t    h차원 -> y차원???
- RNN: F_w는 모두 동일한 weight사용 => 동일한 정보 누적해야 하기 때문에
	1. 가중치 달라지면 그 시점의 입력만 반영하여 학습 가능???
- 다대일: 히든 값만 변경해서 학습
- 다대다: 각 히든마다 출력을 해가며 학습
- L=로스/y1과 정답 비교해서 Loss생성함
- 일대다: 초기 정보만 활용하여 계속 출력
	- 입력 형식이 일정해야함
	- f(x1,h)이기 때문에 의미없는 값(x)을 계속 넣어줘야함
	- 의미없는 값: 더미 벡터
	- 방법1
	- 방법2. 이전 출력을 입력으로 사용(but,출력이 틀릴 수 있기 때문에 정답값을 넣어 줄 수도 있음)
- 시퀀스 투 시퀀스
	- 인코더: 앞부분(고정된 길이로 임베딩(압축))/잠재 표현 벡터/모든 정보를 한개로 요약
	- 언제: 번역할때 많이 사용됨
	- 인코더: 압축/디코더: 해석
	- 인코더,디코더 각자 다른 w사용해야함
	- 예시: one hat인코딩 이용
	- 디코더: 해석 및 출력/다대일(인코더) + 일대다(디코더)
---
예시)4개의 입력 -> 숫자로 변환 작업(임베딩)(대표적인 예: 원햇 인코딩)
	-> 예측('e') -> 예측값('e')을 입력으로 예측 -> ... 반복
	특수 토큰: 해당 문자 출력하면 끝내도록(end token)
예시2) 이미지 캡셔닝(이미지 설명): CNN + RNN
	- 이미지 해석 => CNN사용/텍스트 출력 => RNN 사용
	- 멀티 모달: 여러개의 모델 사용하는 것
	- `<start>` Token이 들어오기 시작
31. V = CNN의 정보/결과: y_0을 예측,y_0을 다음 입력으로
32. `<end>`들어오면 종료
예시3) 시각 질의 응답: 이미지에 대한 대답
	위: 이미지 정보 요약
	아래: 글 정보 요약
	두개의 정보를 합하거나 곱해서 융합

## 순환 신경망 역전파 과정(BPTT)
- 모두 같은 w를 사용하기 때문에 연속 미분 형태가 아니다
- **시점에 따라서 역전파가 일어난다**-> 순서에 따른 기울기 소실 문제 발생
- 현재의 w에 대해서만 가중치를 학습하면 됨
- w_h-> 은닉층
- w_h에 대한 가중치가 계속 들어가기 때문에 중요
- w_hh: 은닉층안에서 다음 입력으로 들어가는 가중치
- w_hh: 모든 시점에 관여
- t에 대한 loss를 미분
- 가장 처음에 h1에 영향을 줌
- tanh의 기울기 소실 문제 발생(p41): 처음 시점까지 못감 => 과거의 정보 잊어버림
	- 활성화함수 사용하지 않거나 다른 활성화 함수 사용해도 모두 기울기 소실이나 폭증 문제가 발생한다
- 장기 의존성 문제: 과거 문제를 잊어버림-> 장기적 의존관계를 학습하지 못함
	- 해결 방법: LSTM

### LSTM
LSTM: 장기 의존성 문제를 게이트로 완화홤
- 입력,출력,망각 게이트를 통해 선택적으로 기억함
- 게이트와 셸 상태 도입
- 히든 상태: 단기 정보 기억
- 셸 상태: 장기 정보 기억
- 게이트: 얼만큼 기억할 건지 결정(0과 1사이의 값이 나옴)
	- 장기 정보 기억이 주 목적
- 게이트 종류
	- 입력: 새 정보 얼마나 반영할 건지
	- 망각: 이전 정보 얼마나 반영할건지
	- 출력: 장기기억을 얼마나 반영할지 + 입력 후보
- g(입력 후보): 새로운 정보 얼마나 가져갈지/tanh사용/x_t와 h_t-1가 입력,바이어스는 b_g
- i,f,o => 얼마나 반영할지 결정
- i(입력 게이트): 시그모이드 사용/g에 i를 곱해서 얼마나 반영할지(입력후보를 얼마나 반영할지)
- f(망각 게이트): 
- o(출력 게이트): 이전 기억을 얼마나 유지,삭제 할지 결정