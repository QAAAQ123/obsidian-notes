LeNet-5: 손글씨 숫자 인식용으로 제작된 가장 기본적인 CNN구조
현대 CNN의 프로토타입
MNIST 데이터셋 사용
1000개의 데이터만 샘플링하여 10번하여 1만번만 하는 이유: python과 numpy만 이용하는데, 이게 CPU연산만 지원하기 때문에 너무 느려서
실제로 소프트맥스를 사용하는데 max를 사용하는 이유: x값이 너무 커지면 오버플로우가 발생할 수 있어서
## 과정
1. Convolutional Layer
	1. 필터 6개
	2. 크기 5x5
	3. 스트라이드=1
2. Average Pooling layer
	1. 크기 2x2
	2. 절반으로 줄어듬
3. Convolutional Layer
	1. 필터 16개
	2. 크기 5x5
	3. 스트라이드=1
4. Average Pooling layer
	1. 크기 2x2
5. fully connected layer
	1. 최종 분류 연산

#### MNIST 데이터 셋
- 손글씨 이미지 데이터 셋

### 기본 함수
- relu
- relu_grad: 미분한 relu
- softmax: 오버플로우 막기 위해서 max(x) 빼줌
- cross_entropy: 손실 함수
- one_hot: 원 핫 인코딩,단위 행렬 생성

## 합성곱
- 초기화
	- 출력,입력,커널 사이즈
- 순전파
	- 출력값 out
	- x: 배치 단위의 학습
	- w: 4개의 차원
	- output 크기=공식으로 계산
	- 빈 텐서 생성후 업데이트
	- 연산
		- 특정 배치,특정 출력 채널,i,j(위치 출력값)
			- 합성곱 연산에 의해서 결정됨
- 역전파
	- dx: 역전파가 뒤에서 앞으로 진행 될 때,출력에 해당하는 dx값이 이전 층의 입력 값으로 전달됨
	- `dW[oc,ic] += dout[b,oc,i,j] * self.x[b,ic,i:i+kH,j:j+kW]`: 전체 가중치 값 계산
		- `dout[b,oc,i,j]`: 이미 이전 층에서 계산 돼서 알고 있는 값
		- `self.x[b,ic,i:i+kH,j:j+kW]`: 입력값에 해당
		- 입력 채널에 해당하지 않는 배치 i,j에 대한 값은 누적을 시킨다
	- `dx[b,ic,iL`: x에 대한 가중치 값
	- 40분


## Pooling layer
1. 초기화: 커널의 사이즈만 입력 받음
2. 순전파
	1. 각 차원 변수
	2. output크기: 평균 풀링의 경우 커널 사이즈에 반비례
	3. 빈 행렬 생성
	4. 평균 풀링 연산
		1. x범위의 행렬 가져옴
		2. x의 평균값 계산
	5. output 출력
3. 역전파: 학습 가능한 파라미터가 없다
	1. 다음 층으로 전달해주는 역할만 한다

## FCL
1. 초기화: 입력차원,출력 차원
	1. w,b 초기화
2. 순전파
	1. `x*W + b`
3. 역전파
	1. 파라미터: w,x,b
	2. 각 파라미터에 대한 미분 값을 구해서 다음으로 넘겨야 한다

### 최종 LeNet5
1. 초기화
2. 순전파
3. 역전파
---
## RNN 코드 구현
- 텍스트 데이터 셋 사용
- 단어 사전 정의
- 데이터 분할(9:1)

- 기본 함수
	- 원 핫 인코딩
	- 소프트맥스
	- 크로스 앤트로피

- RNN 구조
	- 셀 반복
	- 초기화: 입력,은닉 사이즈 정의

**np.dot()과 @ 연산은 행렬 곱 연산으로 결과가 같다**
역전파가 한번 일어날때마다 한칸씩 앞으로 이동한다

FCL은 CNN과 똑같음

## RNN 전체 모델 정의
1. rnn
2. fc