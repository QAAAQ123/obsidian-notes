### **Week 5: 최적화 주관식 퀴즈 (10문항)**


SGD,AdaGrad,RMSprop차이점
SGD: 모든 파라미터에 동일한 학습률
AdaGrad: 각 파라미터에 다른 학습률 적용
RMSprop: 다른 학습률 적용하고, 최근 기울기를 더 크게 반영함

**1. 문제** 경사 하강법의 세 가지 방식(배치, 확률적, 미니배치) 중, '미니배치 경사 하강법'이 현대 딥러닝에서 가장 널리 사용되는 이유를 '안정성'과 '효율성'의 관점에서 설명하시오.
	배치와 확률적 경사 하강법의 중간에 위치하고 있어 2개의 장단점을 효율적으로 받아들어 안정적으로 학습이 가능한 방식이다

**2. 문제** 모멘텀(Momentum) 최적화 기법은 경사 하강법에 '관성'의 개념을 도입합니다. 이 '관성'이 학습 과정에서 어떤 두 가지 긍정적인 역할을 하는지 서술하시오.
	학습의 속도를 증가시키고, 지역 최적해를 탈출 시킬 확률이 올라간다

**3. 문제** 네스테로프 모멘텀(NAG)은 일반 모멘텀과 달리 '한 걸음 먼저 가본 뒤 기울기를 계산'합니다. 이러한 방식이 일반 모멘텀에 비해 갖는 장점은 무엇인지 설명하시오.
	글로벌 손실함수의 최적해를 넘어가게되는 문제를 완화시켜준다

**4. 문제** AdaGrad 최적화 기법의 핵심 아이디어인 '적응적 학습률(Adaptive Learning Rate)'이란 무엇인지, 그리고 어떤 파라미터의 학습률이 커지고 어떤 파라미터의 학습률이 작아지는지 설명하시오.
	모멘텀과 달리 개별적으로 학습률을 다르게 적용한다. 기울기가 작은 파라미터의 학습률이 커지고 기울기가 큰 파라미터의 학습률이 작아진다

**5. 문제** AdaGrad는 학습이 오래 진행되면 학습률이 0에 가까워져 학습이 멈추는 문제가 있습니다. RMSprop은 이 문제를 해결하기 위해 어떤 방법을 사용했는지 설명하시오.
	과거 이동 평균이 아닌 현재의 학습에 더 비중을 두어 학습을 진행한다.❌

**6. 문제** Adam 최적화 알고리즘은 어떤 두 가지 기법의 장점을 결합한 것인지 서술하시오.
	모멘텀과 RMSprop

**7. 문제** 확률적 경사 하강법(SGD)은 배치 경사 하강법(BGD)에 비해 '지역 최솟값(Local Minima)'에서 탈출할 가능성이 더 높습니다. 그 이유는 무엇인지 설명하시오.
	노이즈가 BGD보다 많다. 노이즈가 있기 때문에 튀는 값이 있여 지역 최솟값을 탈출하기 상대적으로 쉽다

**8. 문제** 학습률 스케줄링 기법 중 '웜업(Warmup)'은 학습 초반에 사용됩니다. 이 기법의 목적과 작동 방식을 설명하시오.
	초반에 매우 낮은 학습률을 적용하고 증가시키는 방법,초기 가중치가 랜덤인데, 가중치 랜덤의 문제인 학습의 불안정성을 해결해주는 방안

**9. 문제** Adam 알고리즘에서 '편향 보정(Bias Correction)' 단계가 학습 초반에 특히 중요한 이유는 무엇인지 서술하시오.
	❌

**10. 문제** '학습률 감쇠(Learning Rate Decay)' 기법들이 공통적으로 추구하는 목표는 무엇인지, 학습의 '초반'과 '후반'으로 나누어 설명하시오.
	초반에는 크게 해서 빠르게 학습하고 후반에는 낮춰서 공통 최적해를 잘 찾아갈 수 있게 


### **5번 문제: RMSprop의 원리**

- **제출 답안:** 과거 이동 평균이 아닌 현재의 학습에 더 비중을 두어 학습을 진행한다.
    
- **오답 이유:** 개념적으로는 비슷하지만, RMSprop의 핵심적인 작동 방식을 설명하기에는 다소 부정확한 표현입니다. AdaGrad 역시 '과거'의 정보를 사용하며, RMSprop은 단순히 '현재'에만 비중을 두는 것이 아니라 과거의 정보를 '어떻게' 다룰지를 바꾼 것입니다. 😥
    
- **보충 설명:** AdaGrad는 과거의 모든 기울기 제곱 값을 **영원히 동일하게 축적**하기 때문에 분모가 계속 커져 학습률이 0이 되는 문제가 있었습니다. RMSprop은 이 문제를 해결하기 위해 **지수 이동 평균(Exponential Moving Average)**을 사용합니다. 이는 **최근 기울기에는 높은 가중치**를, **오래된 기울기에는 낮은 가중치**를 부여하여 과거의 정보를 점차 "잊게" 만드는 효과가 있습니다. 따라서 분모가 무한정 커지는 것을 방지하고, 학습률이 0으로 수렴하지 않아 학습을 끝까지 지속할 수 있게 됩니다.
    

---

### **9번 문제: Adam의 편향 보정**

- **제출 답안:** (미응답)
    
- **해설:** Adam 알고리즘은 모멘텀(1차 모멘트, mt​)과 RMSprop(2차 모멘트, vt​)의 이동 평균을 계산합니다. 이 평균값들은 처음에 0으로 초기화됩니다.
    
    학습 초반에는 과거 데이터가 거의 없기 때문에, 이 평균값들이 실제 기울기 평균보다 **0에 가깝게 계산되는 통계적 편향(bias)**이 발생합니다. **편향 보정(Bias Correction)**은 이렇게 0쪽으로 치우친 초기 평균값들을 보정용 수식으로 나누어줌으로써, 실제 평균값에 더 가깝게 만들어주는 단계입니다. 이 과정을 통해 학습 초반부터 더 정확한 모멘텀과 학습률을 적용하여 안정적인 학습을 할 수 있게 됩니다. ✨