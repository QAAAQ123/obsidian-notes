**1. 문제** 다층 퍼셉트론(MLP)에 비선형 활성화 함수를 사용하지 않고 선형 활성화 함수만 사용한다면, 여러 층을 쌓는 것이 왜 의미가 없어지는지 그 이유를 설명하시오.
	선형 활성화 함수를 계속 쌓아가면서 학습해도 결국 최종 결과는 선형 활성화 함수가 되기 때문에, 단층 퍼셉트론과 다를바가 없어진다
**2. 문제** 다층 퍼셉트론의 학습 과정에서 '역전파(Backpropagation)' 알고리즘의 핵심 목표와 정보가 전달되는 방향을 서술하시오.
	역전파는 각 층의 가중치가 오차에 얼마나 많은 기여를 했는지를 파악해서 기여도를 바탕으로 가중치를 수정하는 알고리즘이다

**3. 문제** 다층 퍼셉트론을 구성하는 입력층, 은닉층, 출력층의 역할은 각각 무엇인지 설명하시오.
	입력층은 입력을 받주는 역할,은닉층은 활성화 함수나 가중치를 통해서 학습을 복잡하고 깊게 하는 역할,출력층은 은닉층의 결과를 바탕으로 최종 결과를 계산하는 역할을 한다

**4. 문제** 역전파는 '기여도 할당 문제'를 해결하기 위한 알고리즘입니다. 여기서 말하는 '기여도 할당 문제'란 무엇인지 설명하시오.
	순전파로 진행을 하면서 각각의 가중치가 오차에 얼마나 많은 기여를 했는지 파악하기 어려운 문제

**5. 문제** 경사 하강법(Gradient Descent)은 손실 함수를 최소화하기 위해 사용됩니다. 이 방법이 최적의 가중치를 찾아가는 원리를 '기울기'라는 단어를 사용하여 설명하시오.
	활성화 함수의 기울기를 기반으로 가중치를 업데이트하여 손실 함수를 최소화 하는 방향으로 최적의 가중치를 찾아간다.❌

**6. 문제** 역전파에서 '연쇄 법칙(Chain Rule)'이 필수적으로 사용되는 이유는 무엇인지 개념적으로 설명하시오.
	이전 층의 결과를 다음 층에 반영해야 하기 때문에 ❌

**7. 문제** 시그모이드(Sigmoid) 활성화 함수는 입력값의 절댓값이 매우 커질 경우 학습에 어떤 문제를 일으킬 수 있는지 서술하시오.
	매우 커질 경우 기울기가 0이 되어서 기울기 소실 문제가 발생 할 수 있다.

**8. 문제** 신경망 학습 과정에서 '순전파(Forward Propagation)' 단계와 '역전파(Backpropagation)' 단계의 목적은 각각 무엇인지 비교하여 설명하시오.
	순전파는 학습을 진행하는 목적이고 역전파는 가중치를 업데이트 하는 목적이다 ❌

**9. 문제** 단층 퍼셉트론은 풀 수 없었던 XOR 문제를 다층 퍼셉트론은 해결할 수 있습니다. 다층 퍼셉트론이 이것을 가능하게 하는 원리를 '결정 경계'의 관점에서 설명하시오.
	MLP는 활성화 함수를 통해서 비선형 결정경계를 만들어 줄 수 있는데, XOR은 비선형 문제이기 때문에 XOR은 비선형 결정경계를 이용해 해결 할 수 있다.

**10. 문제** 다층 퍼셉트론의 학습 과정에서 '손실 함수(Loss Function)'의 역할은 무엇인지 설명하시오.
	매 학습마다 얼마만큼의 손실이 발생했는지 확인하는 역할

### **5번 문제: 경사 하강법의 원리**

- **제출 답안:** 활성화 함수의 기울기를 기반으로 가중치를 업데이트하여 손실 함수를 최소화 하는 방향으로 최적의 가중치를 찾아간다.
    
- **오답 이유:** 설명이 약간 부정확합니다. 경사 하강법은 '활성화 함수'의 기울기가 아니라 **'손실 함수(Loss Function)'의 기울기**를 기반으로 가중치를 업데이트합니다. 😥
    
- **보충 설명:** 모델의 최종 목표는 **손실(오차)을 줄이는 것**입니다. 따라서 "현재 가중치를 어느 방향으로 움직여야 손실이 가장 빠르게 줄어들까?"라는 질문에 답해야 합니다. 이 '방향'을 알려주는 것이 바로 **손실 함수를 각 가중치로 편미분한 값**, 즉 **기울기**입니다. 활성화 함수의 기울기는 이 전체 기울기를 계산하는 과정(연쇄 법칙)의 일부일 뿐, 학습 방향을 결정하는 기준 그 자체는 아닙니다.
    

---

### **6번 문제: 연쇄 법칙의 역할**

- **제출 답안:** 이전 층의 결과를 다음 층에 반영해야 하기 때문에
    
- **오답 이유:** 설명이 너무 포괄적이고, '역전파'에서의 역할을 정확히 설명하지 못했습니다. 제출하신 내용은 순전파(Forward Propagation)의 과정에 더 가깝습니다. 🧐
    
- **보충 설명:** 신경망은 입력부터 출력까지 여러 함수가 겹겹이 쌓인 **'합성 함수'**와 같습니다. **연쇄 법칙(Chain Rule)**은 이 복잡한 합성 함수의 미분값을 효율적으로 계산하는 수학적 방법입니다. 역전파에서는 최종 오차에 대해 깊은 곳에 있는 가중치가 얼마나 영향을 미쳤는지(기여도, 즉 기울기)를 알아내야 합니다. 연쇄 법칙은 출력층에서부터 한 단계씩 거꾸로 미분값을 곱해나감으로써, 이 복잡한 계산을 가능하게 해주는 핵심적인 도구입니다.
    

---

### **8번 문제: 순전파와 역전파의 목적**

- **제출 답안:** 순전파는 학습을 진행하는 목적이고 역전파는 가중치를 업데이트 하는 목적이다
    
- **오답 이유:** 각 단계의 목적이 명확하게 구분되지 않았습니다. 특히 역전파의 목적에 대한 설명이 부정확합니다.
    
- **보충 설명:**
    
    - **순전파(Forward Propagation)의 목적:** 입력 데이터를 받아 신경망의 각 층을 순서대로 거치며 최종 ~={red}**예측값을 계산**=~하는 것입니다. '학습을 진행'하는 것의 첫 단계라고 할 수 있습니다.
        
    - **역전파(Backpropagation)의 목적:** 순전파로 얻은 예측값과 실제 정답 사이의 오차를 바탕으로, 이 오차에 각 가중치가 얼마나 기여했는지, 즉 ~={red}**손실 함수에 대한 각 가중치의 기울기를 계산**=~하는 것입니다.
        
    - **가중치 업데이트:** 역전파로 계산된 **기울기**를 사용하여 경사 하강법 공식에 따라 **실제로 가중치를 수정하는 단계**는 역전파 이후에 별도로 이루어집니다. 따라서 "역전파는 가중치를 업데이트한다"기보다는 "역전파는 가중치를 업데이트하기 위한 **기울기를 계산한다**"고 하는 것이 더 정확한 표현입니다. ✨