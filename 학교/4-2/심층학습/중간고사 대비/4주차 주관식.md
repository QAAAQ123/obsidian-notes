### **Week 4: 최적화 주관식 퀴즈 (10문항)**

**1. 문제** 모델이 너무 단순하여 훈련 데이터의 패턴조차 제대로 학습하지 못하는 상태를 무엇이라고 하며, 이때 훈련 성능과 일반화 성능은 각각 어떻게 나타나는지 설명하시오.\
	과소적합이라고 하며,훈련성능과 일반화 성능 모두 낮다

**2. 문제** 신경망 모델의 '깊이(depth)'와 '폭(width)'은 각각 무엇을 의미하며, 과소적합 상태의 모델을 개선하기 위해 이들을 어떻게 조절해야 하는지 서술하시오.
	깊이는 더 복잡한 데이터를 학습 시키는 능력이고, 폭은 특징을 얼마나 잘 추출하는지이다. 깊이와 폭 모두 증가시켜야 한다.❌

**3. 문제** 모델의 '파라미터(parameter)'와 '하이퍼파라미터(hyperparameter)'의 가장 큰 차이점은 무엇인지, 각각의 예시를 하나씩 들어 설명하시오.
	파라미터는 학습 과정에서 자동으로 수정되는 값이고, 하이퍼파라미터는 학습 전에 사용자가 직접 넣어주는 값이다. 파라미터는 가중치,하이퍼파라미터는 학습률이 있다

**4. 문제** 경사 하강법에서 학습률(Learning rate)이 너무 클 때와 너무 작을 때, 손실 함수의 최솟값을 찾아가는 과정에서 각각 어떤 문제가 발생하는지 설명하시오.
	학습률이 너무 크면 이동이 너무 빨라서 손실 함수의 최솟값을 뛰어 넘어 버릴수가 있고, 너무 작으면 최솟값을 찾기 이전에 학습을 멈추거나 지역 최소해에서 멈출 수가 있다.

**5. 문제** 딥러닝 학습에서 '에포크(Epoch)'와 '배치 크기(Batch size)'는 각각 무엇을 의미하는지 정의하시오.
	Epoch은 전체 데이터를 몇번 학습할지 결저하는 거고, 배치 크기는 학습 한번 할때 얼마나 많은 데이터를 넣어줄건지이다.

**6. 문제** 우리가 실제 데이터 분포 전체를 알 수 없기 때문에, 그것을 대신하여 훈련 데이터의 평균 손실을 최소화하는 것을 목표로 삼는 원칙을 무엇이라고 하는지 서술하시오.
	경험적 위험 최소화라고 한다. ❌

**7. 문제** '손실 함수(Loss Function)'와 '비용 함수(Cost Function)'는 어떤 차이가 있는지 '범위'의 관점에서 설명하시오.
	손실함수는 학습 한번에서 나오는 손실이고 비용 함수는 전체 학습 과정에서 나오는 손실이다.❌

**8. 문제** 다중 클래스 분류 문제에서, 모델의 최종 출력층에 소프트맥스(Softmax) 함수를 사용하는 주된 이유는 무엇인지 설명하시오.
	z나 활성화 함수전의 결과 데이터를 확률분포를 따르는 확률로 변환하기 위해서 

**9. 문제** 평균 제곱 오차(MSE)와 교차 엔트로피(CE) 손실 함수는 각각 어떤 종류의 문제(회귀, 분류)에 주로 사용되는지, 그 이유와 함께 서술하시오.
	MSE는 회귀문제,CE는 분류문제이다. MSE는 손실함수의 기울기가 완만하게 감소하기 때문에 연속적인 값을 학습하기에 좋고 CE는 기울기가 급격하게 증감하기 때문에 데이터가 멀리 떨어져있는 특징을 추출하기에 좋다❌

**10. 문제** 분류 문제에서 교차 엔트로피(CE)를 사용했을 때, 모델이 정답을 매우 낮은 확률로 예측하는 큰 실수를 저지를 경우 손실 값이 급격히 커지는 이유는 무엇인지 설명하시오.
	CE는 로그 함수를 사용하기 때문에 log의 특성에 의해 log안에 들어가는 값이 작아질 수록 더욱 빠르게 증가하여 손실이 급격하게 커진다.❌

## **Week 4: 최적화 주관식 오답 노트 ✍️**

### **2번 문제: 신경망 모델의 '깊이'와 '폭'**

- **제출 답안:** 깊이는 더 복잡한 데이터를 학습 시키는 능력이고, 폭은 특징을 얼마나 잘 추출하는지이다. 깊이와 폭 모두 증가시켜야 한다.
    
- **오답 이유:** 각 용어의 의미를 조금 더 명확하게 할 필요가 있습니다. 현재 설명은 다소 추상적입니다. 😥
    
- **보충 설명:**
    
    - **깊이(Depth):** 신경망의 **은닉층 개수**를 의미합니다. 층이 깊어질수록 더 **복잡하고 추상적인(위계적인) 패턴**을 학습하는 데 유리합니다. (예: 점 -> 선 -> 면 -> 형체) 1111
        
    - **폭(Width):** 각 은닉층에 있는 **뉴런(노드)의 개수**를 의미합니다. 폭이 넓어질수록 **하나의 층에서 더 다양한 특징**을 동시에 학습할 수 있습니다. 2222
        
    - 과소적합 상태에서 "깊이와 폭 모두 증가시켜야 한다"는 해결 방향은 맞습니다.
        

---

### **6번 문제: 경험적 위험 최소화**

- **제출 답안:** 경험적 목표 최소화라고 한다.
    
- **오답 이유:** 용어가 약간 틀렸습니다. '목표'가 아니라 '**위험**'입니다. 🧐
    
- **정확한 용어:** **경험적 위험 최소화(Empirical Risk Minimization, ERM)**라고 합니다. 우리가 경험적으로 가지고 있는 데이터(훈련 데이터)의 위험(평균 손실)을 최소화한다는 의미입니다. 3
    

---

### **7번 문제: '손실 함수'와 '비용 함수'**

- **제출 답안:** 손실함수는 학습 한번에서 나오는 손실이고 비용 함수는 전체 학습 과정에서 나오는 손실이다.
    
- **오답 이유:** '학습 한번', '전체 학습 과정'이라는 표현이 모호하여 범위를 명확하게 설명하지 못했습니다.
    
- **보충 설명:**
    
    - **손실 함수(Loss Function):** **개별 데이터 샘플 하나**에 대한 오차를 나타냅니다. (범위: **샘플 단위**) 4
        
    - **비용 함수(Cost Function):** **데이터셋 전체**(또는 미니배치)에 대한 손실의 **평균 또는 합**을 나타냅니다. (범위: **데이터셋 단위**) 5
        
    - 즉, 여러 데이터 샘플의 '손실'을 모아 평균을 낸 것이 '비용'이 됩니다.
        

---

### **9번 문제: MSE와 CE의 사용 이유**

- **제출 답안:** MSE는 회귀문제,CE는 분류문제이다. MSE는 손실함수의 기울기가 완만하게 감소하기 때문에 연속적인 값을 학습하기에 좋고 CE는 기울기가 급격하게 증감하기 때문에 데이터가 멀리 떨어져있는 특징을 추출하기에 좋다
    
- **오답 이유:** 각 문제에 어떤 함수를 사용하는지는 맞았지만, 그 이유에 대한 설명이 부정확합니다. 기울기의 완만함이나 급격함이 주된 이유는 아닙니다.
    
- **보충 설명:**
    
    - **MSE(평균 제곱 오차):** 예측값과 실제값 사이의 **'거리(오차의 크기)'**를 측정합니다. 주가나 집값처럼 연속적인 값의 차이를 측정하는 **회귀 문제**에 자연스럽게 어울립니다. 6
        
    - **CE(교차 엔트로피):** 두 **'확률 분포'** 사이의 차이를 측정합니다. 모델이 예측한 확률 분포와 실제 정답의 확률 분포(예: [0, 0, 1])가 얼마나 다른지를 측정하므로, 정답이 특정 카테고리일 확률을 예측하는 **분류 문제**에 적합합니다. 7
        

---

### **10번 문제: CE 손실이 급격히 커지는 이유**

- **제출 답안:** CE는 로그 함수를 사용하기 때문에 log의 특성에 의해 log안에 들어가는 값이 작아질 수록 더욱 빠르게 증가하여 손실이 급격하게 커진다.
    
- **오답 이유:** 핵심은 파악하셨지만, 로그 함수의 특성을 반대로 설명했습니다. `log(x)` 함수 자체는 x가 작아질수록 **감소**합니다.
    
- **보충 설명:** 교차 엔트로피는 그냥 로그가 아니라 **마이너스 로그($-log(x)$)**를 사용합니다. `log(x)`는 x가 0에 가까워질수록 **음의 무한대**로 급격히 **감소**합니다. 여기에 마이너스(-)가 붙으면서, **$-log(x)$** 값은 **양의 무한대**로 급격히 **증가**하게 됩니다. 이 원리 때문에 모델이 큰 실수를 하면 손실 값이 기하급수적으로 커지는 강력한 페널티가 부여됩니다. 8888