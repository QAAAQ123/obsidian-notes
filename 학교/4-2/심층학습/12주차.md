LSTM: 셸 상태 경로-기울기 소실 문제 완화
	셸에 대한 미분값
	C_t = f 0 C_t-1+ i 0 g
	L=Loss(손실 함수)
	f는 0-1값을 가짐 => 기울기 소실 문제 없어짐
	이론적으로는 기울기 소실문제 해결하지만, 실제에서는 문제 있음 => 트랜스포머 모델
28. 29. 컨텍스트 벡터x토큰x
29. e
30. e
31. e
32. 한계: 입력이 하나의 c(context vector)로 압축됨
	1. 정보가 사라지는 병목현상 발생
33. 어텐션: 어떤 단어를 생성할 때 전체 입력 중에 관련이 높은 것만 참고하는 것
	1. 병목현상 제거하지만, 연산량이 증가한다
34. e: 인라인먼트 스코어(얼마만큼 관계성을 가지고 있는지)
	1. s와 h를 연결함 -> f_att에 넣은 -> 스칼라 값 = e
35. 인라인먼트--(소프트맥스)--> attention weight
	1. attention weight
		1. s_0와 얼마나 상관있는지 확률로 나타냄
		2. 얼마만큼 참조해서 가져올지(비율)
36. e -> a -> c -> 디코딩 
	1. C: context vector-고정된 크기,중요정보 포함
37. C_t를 항상 계산해야 한다
38. ㄷ
39. ㄷ
40. 어텐션맵을 통해서 관련도 파악
	1. 열: 입력
	2. 행 출력
	3. 똑같은 부분이 출력에 많이 관여 안하고 중간에 복잡한 형태는 3개가 관여한다
41. ㅈ
42. 쿼리 벡터: 검색의 기준(S_0)-S_0과 얼마나 관련있는지 찾았기 때문
	1. 데이터 벡터
	2. 결과 벡터
	3. 입력: s,h[단어개수 x 차원]
	4. 키는 알려주기는 하지만, 데이터는 실제로 데이터를 가지고 있다
	5. 유사성(e) => 스칼라 값

---
수요일 온라인
복습
1. LSTM
	1. RNN의 장기 의존성 문제 해결하기 위한 모델로 게이트와 셸 상태 대입
	2. 입력,망각,출력,입력 게이트
2. 장기 의존성 문제를 완화하는 핵심 구조: 셀 상태 경로
3. 손실함수를 파악하기 위해서 미분을 계속하면 식에 의해서 f_j가 계속 곱해진다. -> 기울기 소실 문제을 완화해준다
	1. **~={red}왜 계속 곱해준는데 기울기 소실이 안돼는건지?(시험)=~** f는 망각 게이트이기 때문에->기존의 기억을 계속 학습하고 싶어하면 f가 1에 가깝게 되기 때문에 계속해서 곱해져도 기존의 정보를 잊어버리지 않기 때문에 기장기 의존성(기울기 소실)문제가 없다
	2. 이전의 정보를 잊어버릴때 f_j는 0에 가깝기 때문에 기존의 정보를 잊어버리게 학습이 된다
	3. 결론: f를 통해서 장기 의존성 문제을 해결 할 수 있기 때문에 잊을지 계속 가지고갈지를 결정 할 수 있게 된다.
4. 역전파 과정에서 f게이트만 지속 누적되는 단순한 구조를 가진다.->Resnet의 구조와 비슷하다
 5. 시퀀스-투-시퀀스
	 1. 인코더,디코더로 구성
	 2. 인코더: 입력 시퀀스의 정보를 고정된 잠재표현인 c로 임베딩
	 3. 잠재 표현을 사용해서 다음에 오는 디코더가 순차적으로 출력 시퀀스를 생성한다
	 4. 일반적으로는 번역에 많이 사용한다. 
	 5. c를 통해서 출력한다
	6. RNN/LSTM의 근본적 한계점
		1. 입력 정보가 고정된 크기의 벡터 C로 압축됨
		2. 시퀀스(입력)가 길어질수록 정보가 사라지는 병목 현상
6. 어텐션: 현재 출력을 생성하기 위해서 **입력 시퀀스를 다시 참조**하는 매커니즘 -> 병목 현상 완화
7. 어텐션 과정
	1. 현재 은닉층:h/과거 은닉층:s/현재,과거 은닉층 관계:e/ 관계e를 소프트맥스: a/최종 결과: c
	2. 입력에 대한 은닉층 값 계산
	3. 과거 은닉층인 s0와 현재 은닉층의 관계를 계산해야함:f(s,h)
		1. 기본적인 계산 방법: s_t-1과 h_i를 연결 -> 이것을 fully connected layer의 입력으로 전달 -> fc layer는 두개가 유사할 수록 더 큰값 출력,다를 수록 낮은 값 출력
		2. 결론: s와 h이 유사할 수록 e값이 크다
	4. e를 소프트맥스를 통해 확률 분포로 변환
	5. a: 어텐션->출력 값을 계산할때,얼만큼의 비율로 반영할지를 나타냄 
	6. 각각의 h와 a를 곱하고 곱한 값을 더하면 입력을 중요도 순으로 반영을 한 한개의 context벡터를 만들수 있다. 
	7. context_0 벡터와 start 토큰을 사용해서 처음 단어를 예측한다
	8. s1의 은닉층 값과 2~6과정을 반복해서 얻어낸 c2를 만든다. c2와 이전 단어(y_1) 토큰과 합쳐서 그 다음 단어(y_2)를 만든다. end 토큰과 마지막 context벡터가 만날때까지 반복한다
8. 어텐션 맵: 어텐션 가중치를 시각적으로 나타난 **가중치 행렬**
9. 용어 정리
	1. 쿼리 벡터: s,과거 은닉층-검색의 기준이 되는 벡터
	2. 데이터 벡터: x,현재 은닉층-입력 정보 벡터
	3. 결과 벡터: c,최종 결과-정보를 반영한 최종 결과 벡터
10. 용어 사용하는 예제
	1. D_q 쿼리벡터의 차원,N_x 입력 시퀀스의 길이,D_x 입력 시퀀스의 차원
	2. 쿼리와 데이터가 얼마나 관련도가 있는지 계산
	3. 어텐션 계산: 소프트맥스
	4. 계산 결과 차원은 N_x의 차원이 된다
	5. 결과 계산: 어텐션과 데이터를 계산하여 아웃풋을 출력한다. 하나의 차원이기 때문에 D_x 차원이 된다.
--- 
수업-기본 모델 발전
~={red}매트릭스 연산을 통해 다양한 벡터를 계산하고, 그 결과로 새로운 벡터가 나오는 구조->매트릭스와 벡터는 다르다=~
1. 유사성(데이터x쿼리 벡터)를 할 때 내적을 사용을 해서 유사성 계산 가능
	1. 쿼리는 고정-> 내적값은 결국 현재 은닉층의 크기와 cos(theta)값에 영향을 받는다
	2. q와 x값이 멀리 떨어져 있다면 세타 값이 커짐->cos 세타 값이 작아짐 -> 전체적인 내적값이 작아짐
	3. q와 x값이 가까이 있다면 세타 값이 작아짐 -> cos 세타 값이 커짐 -> 전체적인 내적값이 커짐
	4. 다른 네트워크를 사용하지 않고도 q와 x간의 유사성을 파악가능하다
2. 쿼리 벡터가 여러개 들어올때도 계산 가능
	1. 셀프 어텐션이 예시 중에 하나
	2. 쿼리 벡터의 일반화된 표현: N_Q개의 쿼리 벡터,D_Q: 각 쿼리벡터의 차원
	3. 데이터 벡터의 기능 2개: 내적->유사성 계산/곱셈->어텐션과 은닉층 곱셈하는 역할,실제 데이터 보관 역할
	4. 2개의 기능에서 서로 필요한 정보가 다를 경우에 각각에 대한 정확도가 떨어짐->key,value 벡터로 역할 분리하여 쪼갬
	5. key: 쿼리 벡터와 내적 연산하여 유사성
	6. value: 실제 값 저장
3. 실제 연산
	1. key: 데이터 벡터와 key matrix를 곱한다(x차원,쿼리 차원)
	2. value: 데이터 벡터와 value matrix를 곱한다(x차원, value 차원)
	3. 유사도
		1. 쿼리 벡터와 키 이용해서 유사도 계산
		2. D_q차원의 루트로 나누는 이유: 내적값을 계산할 때 차원이 커질 수록 크기가 커진다. 따라서 분산도 D_q에 비례하여 커진다.표준편차가 루트D_q이다->**D_q차원에 따른 분산 값을 일정하게 유지시켜줌(소프트맥스 함수가 지수함수를 사용하기 때문에)**
		3. 결과 벡터: 시그마(어텐션 weight X value 벡터)
4. 크로스 어텐션
	1. 두 다른 입력간의 입력을 참조해서 필요한 정보를 참조하기 위한 어텐션 메커니즘
	2. x,q 벡터가 들어옴
	3. x(데이터 벡터)로부터 key,value 벡터를 계산
	4. 쿼리와 key벡터의 유사성을 계산해서 e 벡터 계산(행렬곱으로 한번에 병렬로 계산됨)
	5. 소프트맥스 함수를 적용해서 a 벡터(어텐션) 벡터 계산
	6. 어텐션 벡터와 V(value) 벡터간의 연산
		1. 열 기준: 얼만큼 유사성을 가지고 있는지
		2. y1은 q1에대한 context 벡터-> 각 쿼리에 대한 context(출력)벡터를 계산 할 수 있다
5. 셀프 어텐션
	1. 하나의 입력에서 서로의 정보를 참조하여 필요한 정보를 선택하는 어텐션 메커니즘
	2. 입력이 하나만 있음=q,k,v 벡터가 모두 x로부터 나온다
	3. 입력에서 q벡터를 계산해야 하기 때문에 query matrix가 추가로 필요하다
	4. 계산
		1. x에 w_q,k,v를 곱해서 벡터 얻음
	5. 목적: 입력값 사이의 유사도 계산
		1. 어떤 단어를 참조하는게 유리할지 판단 할때(어텐션으로 판단)
	6. 계산 요약: `Q*K=E,Q*V=A,X->Q,K,V`
6. 어텐션의 한계점
	1. 순서가 바뀌어도 전체 입력값이 똑같으면 출력값도 바뀌지 않는다(순서만 바뀐다)
	2. 입력의 합이 똑같으면 걸과가 똑같기 때문에 단어의 순서를 반영하지 못한다. -> 순서 정보가 주어지지 않으면 문장 구조에 대한 해석 능력이 없다
7. 어텐션의 해결 방법1: 위치 인코딩을 각 입력에 추가
	1. 다양한 위치 인코딩 함수 사용 가능
8. 해결 방법2: 마스킹된 셀프 어텐션
	1. 현재 위치 이전만 보도록 마스킹함,다음 단어를 보지 못하고 연산함
	2. 주로 디코딩과정에서 사용됨
	3. 실제로는 입력이 한번에 들어오고 한번에 출력됨->방지하기 위해서 마스킹 사용(-무한대로 마스킹함,-무한대를 쓰면 소프트맥스 함수에서 결과가 0이 나옴)
9. 해결방법3-다중 헤드 셀프 어텐션
	1. 여러개의 셀프 어텐션을 병렬로 한번에 수행
	2. 다양한 관계를 동시에 학습하여 풍부한 특징 표현을 생성한다
	3. 모든 관계를 한개의 셀프 어텐션으로 학습하기 어렵기 때문에 각각의 역할을 나누어서 학습한다
	4. 입력
		1. H(Head):하나의 셀프 어텐션을 의미
		2. 전체 차원: D차원
		3. H개의 Head를 사용했기 때문에 D차원으로 유지시키기 위해서는 D차원을 H로 나누어서 차원을 줄여야함
	5. 연산
		1. q,k,v가 D차원으로 줄어듬
	6. 결과
		1. x1입력의 결과: y1,1/y1,2/y1,3을 하나의 출력인 O_1으로 합쳐줌
	7. 전체 연산 과정: 네 가지 행렬 곱셈
		1. QKV Projection: Q,K,V를 구하는 과정
			1. `[N x D][D x 3HD_H] => [N x 3HD_H]`행렬 곱 연산(N: 입력의 개수)
		2. QK 유사성:`[N x N x D_H][H x D_H x N] => [H x N x N]`n개의 입력내에서 어떤 관계를 가지고 있는지 표현
		3. V-Veighting(A x V):`[H x N x N][H x N x D_H] => [H x N x D_H] => [N x HD_H`H개의 헤드가 독립적인 D_H라는 특징을 가지고 있다
		4. output projection: 모든 헤더의 특징 값들을 섞어주는 역할(output matrix사용)
			1. `[N x HD_H][HD_H x D] => [N x D]`