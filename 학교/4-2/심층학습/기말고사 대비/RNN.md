## RNN 구조
1. 일대일
2. 일대다
3. 다대일
4. 다대다
5. 시퀀스-투-시퀀스
	1. 어텐션
6. LSTM
- 은닉 상태

## RNN 알고리즘
- BPTT(시간에 따른 역전파)
## RNN 예시
1. 다음 문자를 예측하는 언어 모델
2. 이미지 캡셔닝
3. 시각 질의 응답

## 어텐션
1. 크로스 어텐션
2. 셀프 어텐션
3. 마스킹된 셀프 어텐션
4. 다중 헤드 셀프 어텐션

---

입력 시퀀스: ==순서가 있는 요소들의 연속적인 나열==
## RNN(순환 신경망)
- 순환 신경망: **이전 단계의 정보를 다음 단계에 전달하는 순환적 구조를 가진 신경망**
- 순서를 가진 형태의 데이터를 처리하기 위해서 고안된 신경망
- MLP,CNN은 순서 관계를 학습하지 못함
	- MLP,CNN은 one to one 형태
- 은닉 상태: 이전 시점의 정보를 인코딩하여 다음 단계로 전달하는 표현 상태
  $$ h_{t} = f_{w}(h_{t-1},x_{t})$$
- 동일한 정보를 누적해야 하기 때문에 동일한 가중치 사용

## RNN 구조
1. 다대일 구조: 입력 시퀀스를 받아 하나의 출력을 생성하는 RNN구조
	1. 히든 값만 변경해서 학습 진행
2. 다대다 구조: 입력 시퀀스를 받아 여러 시점의 출력 시퀀스를 생성하는 RNN구조
	1. 각 은닉층마다 출력을 해가며 학습
3. 일대다 구조: 하나의 입력을 받아 여러 시점의 출력 시퀀스를 생성하는 RNN 구조
	1. 초기 정보만 활용하여 출력
	2. 중간에 계속 더미 벡터를 넣어줘야 한다
4. 시퀀스 투 시퀀스: 입력 시퀀스를 잠재 표현으로 임베딩하고 잠재 표현을 사용해서 출력 시퀀스 생성
	1. **다대일(인코더)과 일대다(디코더) 구조가 결합된 형태**
	2. 잠재표현: 입력 시퀀스가 인코딩 되어 고정된 길이로 압축된 형태
	3. 인코더,디코더 **각자 다른 가중치 사용**해야 한다

다대다(입력=출력길이)->시퀀스 투 시퀀스(인코더+디코더,입력!=출력길이)->어텐션(context벡터의 고정길이 정보 손실 문제 해결)
## RNN 예시
1. 다음 문자 예측
	1. 학습 단계 -> 추론 단계
	2. 순서
		1. 첫번째 문자
		2. one hot 인코딩으로 문자를 숫자로 변형(임베딩)
		3. 인코딩 -> 은닉층 값 생성
		4. 디코딩 -> 은닉층 값을 디코딩 하여 출력
		5. 디코딩 한 값을 소프트맥스
		6. 소프트맥스 결과를 바탕으로 문자로 변환(두번째 문자 예측)
		7. 결과를 다음 입력으로 이용
		8. 임베딩
		9. 인코딩(이전 은닉층 값 활용)
		10. 디코딩
		11. 소프트맥스
		12. 문자로 변환... 반복
2. 이미지 캡셔닝: CNN + RNN
	1. 이미지 해석: CNN
	2. 텍스트 출력: RNN
	3. 멀티 모달: 여러개의 모델 사용하는 것
	4. start 토큰 들어오면 시작,end 토큰 출력되면 끝
3. 시각 질의 응답: 이미지에 대한 대답
	1. 이미지 정보와 글 정보를 따로 처리하고 정보를 합하여 융합
	2. 이미지 해석: CNN
	3. 글 해석: RNN
	4. CNN+RNN 결합 -> Fully connetec layer(MLP) -> Softmax -> 출력

## 시간에 따른 역전파(BPTT)
- BPTT: 시간축을 펼쳐서(unroll) 마치 여러 층의 네트워크처럼 만든 뒤, 그 위에서 역전파를 수행하는 방식
	- Unroll (펼치기): RNN을 여러 시점으로 펼쳐서, 각 시점의 입력과 출력을 연결된 네트워크처럼 표현합니다.
- tanh나 다른 활성화 함수를 사용하면 기울기 소실 문제가 발생 할 수 있다.
	- 기울기 소실문제: 과거의 정보를 잊어버림
- 과거의 정보를 잊어버리는 장기 의존성 문제를 LSTM으로 해결한다

## LSTM
- LSTM: 순환 신경망에서 발생하는 장기 의존성 문제를 **게이트 구조**를 통해 완화한 신경망 
	- 입력 게이트, 망각 게이트, 출력 게이트를 통해 정보를 선택적으로 기억
	- 기울기 소실 문제 감소 및 장기 의존성을효과적으로 학습
- 게이트: LSTM 내부에서 정보의 흐름을 선택적으로 제어하는 모듈
	- 각 게이트는 시그모이드 함수를 이용해서 반영 정도를 0-1사이로 조절
	- 필요한 정보를 유지하고 불필요한 정보를 제거
- ~={orange}셸 상태: 장기 기억을 저장하고 시점 간 정보 전달하는 **장기 기억 저장소**-장기 의존성 문제 해결하는 핵심 게이트=~
- 은닉 상태 ($h_t$): 셀 상태를 가공($tanh$)해서 실제 출력으로 내보내는 값, 단기적인 문맥 정보
- 셸 상태 업데이트 식$$c_{t} = f_{t} \odot c_{t-1} + i_{t} \odot g_{t}$$
- 은닉 상태 출력 식$$h_{t} = o_{t} \odot \tanh(c_{t})$$
$\odot$: 요소별 곱
#### ~={red}LSTM이 장기 의존성 문제를 해결한 방법=~
- 기존의 기억을 계속 학습하고 싶어하면 f가 1에 가깝게 되기 때문에 계속해서 곱해져도 기존의 정보를 잊어버리지 않기 때문에 장기 의존성(기울기 소실)문제가 없다
### LSTM 게이트
- 입력 게이트(Input Gate): 새로운 정보를 얼마나 반영할지 결정
- 망각 게이트(Forget Gate): 이전 기억을 얼마나 유지/삭제할지 결정
- 출력 게이트(Output Gate): **셀 상태를 얼마나 출력으로 노출할지 결정**
- 입력 후보(Candidate Cell State): 셀에 추가할 새로운 정보 생성

### 시퀀스 투 시퀀스
1. RNN/LSTM의 구조적 한계점
	1. 입력 시퀀스의 길이가 고정된 크기인 c로 압축
	2. 시퀀스가 길어질수록 정보가 사라지는 병목 현상
2. 과정
	1. 입력 시퀀스를 인코더가 고정길이의 잠재표현으로 임베딩
	2. 잠재 표현을 사용하여 디코더가 출력 시퀀스 생성
3. 다대일(인코더) + 일대다(디코더) 구조
### RNN/LSTM 한계
- 입력 정보가 고정된 크기의 c벡터로 압축됨
- 시퀀스가 길어질수록 병목 현상
## 어텐션
- 어텐션: 현재 출력을 생성하기 위해서 ~={red}**입력 시퀀스 전체를 다시 참조**=~하는 메커니즘
- 출력 시점마다 입력 전체(은닉층)을 다시 참조하여 고정 크기의 벡터로 압축되는 병목을 제거
- 어텐션 맵(Attention map): 각 입력에 대한 attention weights를 시각적으로 나타낸 가중치 행렬
- 벡터
	1. 쿼리 벡터:현재 시점에서 검색의 기준이 되는 벡터(인코딩 할때 데이터 벡터와 함께 들어가는 벡터)
	2. 데이터 벡터: 검색 대상이 되는 입력 정보 벡터(은닉층)
	3. 결과 벡터: 쿼리가 데이터 벡터에서 필요한 정보를 찾아 반영한 최종 결과 벡터(context 벡터(압축된 고정길이 벡터))
- 계산 값
	- e: 입력 벡터와 쿼리 벡터의 관계 표현(내적)
	- a: e를 소프트맥스를 해서 확률화
	- 어텐션(a)가 끝나고 context 벡터로 가기 전에 은닉층의 데이터 벡터를 한번 더 참조해서 context 벡터를 도출함

## 어텐션 종류
1. 크로스 어텐션: 두 다른 입력 간의 정보를 참조하여 필요한 정보를 선택하는 어텐션 메커니즘
	1. X(데이터)는 K,V가 되고 Q 그대로
	2. X,Q 벡터가 들어옴
	3. 데이터 벡터를 Key,Value 벡터로 변환해서 K는 e에 V는 a에 넣는다
	4. **a에서 c로 갈때 V를 곱한다**
	5. **~={red}D_q차원의 루트로 나누는 이유: 내적값을 계산할 때 차원이 커질 수록 크기가 커진다. 따라서 분산도 D_q에 비례하여 커진다.표준편차가 루트D_q이다->D_q차원에 따른 분산 값을 일정하게 유지시켜줌(소프트맥스 함수가 지수함수를 사용하기 때문에)=~**
2. 셀프 어텐션: 하나의 입력에서 서로의 정보를 참조하여 필요한 정보를 선택하는 매커니즘
	1. X(데이터)에서 Q,K,V 얻음
	2. 데이터 벡터를 Query,Key,Value 벡터로 변환에서 Q,K는 e에 V는 a에 넣는다
	3. 목적: 입력값 사이의 유사도 계산
	4. **a에서 c로 갈때 V를 곱한다**
	5. **~={red}D_q차원의 루트로 나누는 이유: 내적값을 계산할 때 차원이 커질 수록 크기가 커진다. 따라서 분산도 D_q에 비례하여 커진다.표준편차가 루트D_q이다->D_q차원에 따른 분산 값을 일정하게 유지시켜줌(소프트맥스 함수가 지수함수를 사용하기 때문에)=~**
3. 크로스,셀프 어텐션의 한계
	1. 순서가 바뀌어도 전체 입력값이 똑같으면 출력값도 바뀌지 않는다
	2. 입력의 합이 똑같으면 결과가 똑같기 때문에 단어의 순서를 반영하지 못한다
	3. 해결 방법: 위치 인코딩을 각 입력에 추가
4. 해결방법1: 위치 인코딩을 각 입력에 추가
5. 마스킹된 셀프 어텐션
	1. 현재 위치 이전만 보도록 마스킹된 셀프 어텐션
	2. 벡터가 이후 위치의 정보를 미리 참조하지 못하도록 방지
	3. 음의 무한대를 넣으면, 소프트맥스에서 0이 나오도록 함
6. 다중 헤드 셀프 어텐션
	1. 여러개의 셀프 어텐션을 병렬로 수행하는 어텐션 구조
	2. 다양한 관계를 동시에 학습하여 풍부한 특징 표현을 생성한다
	3. 모든 관계를 한개의 셀프 어텐션으로 학습하기 어렵기 때문에 각각의 역할을 나누어서 학습한다
	4. H개의 Head를 사용했기 때문에 D차원으로 유지시키기 위해서는 D차원을 H로 나누어서 차원을 줄여야함



---


## 1. 순환 신경망 (Vanilla RNN)

**"순서가 있는 데이터를 처리하자!"**

- **핵심 개념:** 이전 시점($t-1$)의 정보를 은닉 상태($h_{t-1}$)에 담아 현재 시점($t$)으로 전달하여 **시간적 의존성(Temporal Dependency)**을 학습합니다 1.
    
- **구조:** 입력($x_t$)과 이전 은닉 상태($h_{t-1}$)를 결합하여 현재 은닉 상태($h_t$)를 갱신합니다.
    
    - 수식: $h_t = \tanh(W_{hh}h_{t-1} + W_{xh}x_t)$2.
        
- **특징:** 모든 시점에서 **가중치($W$)를 공유**합니다3333.
    
- **한계 (★시험 출제):** **장기 의존성 문제 (Long-term Dependency)**
    
    - 역전파 시 그래디언트가 계속 곱해지면서 사라지거나(**Vanishing**) 폭발(**Exploding**)하여, 먼 과거의 정보를 기억하지 못합니다4444.
        


## 2. LSTM (Long Short-Term Memory)

**"중요한 정보는 고속도로를 태워 오래 기억하자!"**

- **등장 배경:** RNN의 기울기 소실 문제를 해결하기 위해 등장했습니다 5.
    
- **핵심 구조:**
    
    - **셀 상태 ($C_t$):** 정보가 변질되지 않고 흐를 수 있는 '고속도로' 역할을 하며 장기 기억을 담당합니다6.
        
    - **게이트 (Gate):** 시그모이드($\sigma$)를 통해 정보의 흐름을 제어합니다 7.
        
        1. **망각 게이트 (Forget):** 과거 정보를 얼마나 지울지 결정8.
            
        2. **입력 게이트 (Input):** 현재 정보를 얼마나 저장할지 결정9.
            
        3. **출력 게이트 (Output):** 현재 셀 상태를 바탕으로 은닉 상태($h_t$)를 얼마나 내보낼지 결정10.
            


## 3. Seq2Seq (Sequence-to-Sequence)

**"입력을 다 읽고 나서 출력을 만들자!" (번역기 구조)**

- **구조:** **인코더(Encoder)**가 입력 시퀀스를 읽고, **디코더(Decoder)**가 출력 시퀀스를 생성합니다11111111.
    
- **동작:** 인코더의 마지막 은닉 상태(Context Vector)가 디코더의 초기 상태로 들어갑니다.
    
- **한계 (★병목 현상):** 입력 문장이 아무리 길어도 **고정된 크기의 벡터 하나**에 모든 정보를 압축해야 하므로 정보 손실이 발생합니다12121212.
    


## 4. 어텐션 (Attention) 메커니즘

**"다 외우지 말고, 필요할 때마다 컨닝(참조)하자!"**

- **핵심 아이디어:** 디코더가 출력을 생성할 때마다, 인코더의 **모든 입력 단어들을 다시 참고(Attend)**합니다13. 이때 중요한 단어에 더 많은 가중치(Attention Score)를 둡니다.
    
- **구성 요소:**
    
    - **Query (Q):** "나 지금 이거 궁금해" (현재 디코더의 은닉 상태)14.
        
    - **Key (K):** "내용의 라벨/키워드" (인코더의 은닉 상태들)15.
        
    - **Value (V):** "실제 내용물" (인코더의 은닉 상태들, 보통 K=V)16.
        
- **효과:** 고정된 벡터에 정보를 욱여넣을 필요가 없어 **병목 현상이 해결**되고 긴 문장 번역 성능이 비약적으로 상승합니다.
    


## 5. 셀프 어텐션 (Self-Attention) & 트랜스포머

**"RNN 없이 어텐션만으로 문장을 이해하자!"**

이 부분이 가장 헷갈리기 쉽습니다. 일반 어텐션(Cross-Attention)과의 차이를 명확히 하세요.

### ① 셀프 어텐션 (Self-Attention)

- **개념:** **입력 문장 내의 단어들끼리** 서로 어떤 관계가 있는지 계산합니다17171717. (예: "The animal didn't cross the street because **it** was too tired."에서 'it'이 'animal'을 가리키는지 알아내는 과정)
    
- **특징:** $Q, K, V$가 모두 **동일한 입력 벡터 $X$**로부터 생성됩니다 ($Q=XW_Q, K=XW_K, V=XW_V$) 18181818.
    

### ② 마스킹된 셀프 어텐션 (Masked Self-Attention)

- **위치:** 트랜스포머의 **디코더(Decoder)**에서 사용됩니다19191919.
    
- **이유:** 학습 시 정답을 미리 알고 있지만, 모델은 **미래의 단어(아직 생성되지 않은 단어)를 봐서는 안 됩니다.** 따라서 현재 위치 이후의 정보는 $-\infty$로 마스킹하여 Softmax 결과가 0이 되도록 합니다 20202020.
    

### ③ 다중 헤드 어텐션 (Multi-Head Attention)

- **개념:** 어텐션을 **여러 개(Head) 병렬로 수행**하는 것입니다 21.
    
- **이유:** 문장의 **다양한 특징**을 동시에 포착하기 위해서입니다. (예: 헤드 1은 문법적 관계, 헤드 2는 의미적 관계, 헤드 3은 위치적 관계 등) 22.
    
- **연산:** 입력 차원을 헤드 수($H$)로 나누어($D/H$) 각각 어텐션을 수행한 후, 다시 합칩니다(Concat) 23.
    


## 6. 교수님의 시험 총정리 (Checklist)

시험장에 들어가기 전, 다음 질문에 답할 수 있는지 확인하세요.

1. **RNN vs LSTM:** $C_t$의 존재 유무와 그 역할(고속도로)을 설명할 수 있는가?
    
2. **Seq2Seq의 한계:** '고정 길이 벡터(Context Vector)'가 왜 병목인지 설명할 수 있는가?
    
3. **Attention의 Q, K, V:** 번역기(Cross-Attention)에서 Q는 어디서 오고, K와 V는 어디서 오는지 구분할 수 있는가? (Q: 디코더, K,V: 인코더)
    
4. **Self-Attention:** $Q, K, V$가 어디서 오는지(입력 $X$ 자신), 그리고 왜 **병렬 처리**가 가능한지 설명할 수 있는가?
    
5. **Masked Attention:** 왜 디코더에서는 미래의 정보를 가려야(Masking) 하는가?
    

여기까지가 RNN부터 트랜스포머의 핵심 기술까지의 요약입니다. 이 흐름을 이해하면 딥러닝의 가장 중요한 챕터를 마스터한 것입니다. 건승을 빕니다!