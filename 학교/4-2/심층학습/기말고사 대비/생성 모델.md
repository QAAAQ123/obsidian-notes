## 생성 모델
1. 생성 모델 vs 판별 모델
	1. 생성 모델: 데이터의 분포 모델링
		1. 명시적 밀도 추정: 확률 밀도 함수 명시적 정의
		2. 암시적 밀도 추정: 확률 밀도 함수 명시적 정의하지 않음
	2. 판별 모델: 결정 경계 모델링
2. 생성 모델 종류
	1. 명시적 밀도 추정
		1. PixelRNN: RNN기반 순차적 생성
		2. PixelCNN: 마스킹된 CNN 필터 기반 순차적 생성
		3. PixelRNN,CNN 단점
			1. 순차 생성 -> 병렬처리 불가
			2. 장기 의존성 학습 못함
			3. 깊은 모델이 필요해서 메모리,연산량 증가
		4. 오토인코더(AE)
			1. 인코더(CNN구조) -> 잠재표현(z) -> 디코더(전치 합성곱)
			2. 핵심 특징 학습하고 원본 데이터 복원
			3. Fully connected layer 이용해서 차원 압축
			4. 미세조정하여 다운스트림
			5. **전치 합성곱 이용해서 복원**
			6. 생성 모델이 아님
			7. 클래스를 분류하는 계층이 없기 때문에 비지도 학습이다
		5. 변분 오토인코더(VAE)
			1. 인코더 -> 잠재 표현 공간(표준 정규 분포) -> 샘플링 -> 디코더
			2. 정규분포 기반으로 연속 확률 모델링
			3. 평균값만 사용
			4. **오토인코더와 차이점: 잠재 공간을 정규 확률 분포로 표현**
				- 정규 확률 분포를 알기 위해서는 확률과 분산을 알고 있어야 한다
				- 그래서 잠재 공간에서는 각 차원마다 평균,분산 값을 가진다
				- 차원이 10개이더라도 실제로는 평균,분산 값을 가져야 하기 때문에 20개의 값을 가진다(각 차원의 분산,각 차원의 평균)
			5. ELBO: 데이터 분포 근사
				1. 재구성 항: 잠재 변수로부터 원본을 얼마나 잘 복원하는지 측정
					1. 인코더,디코더,디코더 출력
				2. 정규화 항: 잠재공간이 일정한 분포 유지하도록 함
					1. 잠재 공간의 정규 분포
					2. 입력 x와 잠재 표현 z의 차이가 작아질 수록 KL이 작아짐
					3. 출력과 예측 확률 분포의 차이
					4. 실제 출력과 예측이 같으면 0이 된다
					5. 불일치가 커질수록 KL값이 커진다
				3. log >= 재구성항 - 정규화항이다
					1. 재구성 항이 커지거나 정규화 항이 작아지게 학습한다
			6. 재파라미터화 트릭: 확률 분포를 미분 가능하도록 만듦
				- 입실론: 정규분포에서 샘플링된 값
				- 수식으로 표현(결과 값으로 표현)
				- 기존: z-랜덤/평균과 분산-확정
				- 이후: z,평균,분산-확정/입실론-랜덤
				- 입실론은 역전파가 안돼도 상관없다
			7. 실제에서 디코더는 평균값만 구한다. -> 디코더의 평균만 가지고 x_hat을 생성한다
				- 이유1. 학습의 안정성을 높히기 위해서
				- 이유2. 불필요한 노이즈 감소를 위해서
			8. 한계점
				1. 복잡한 데이터를 명시적 모델링 하기 어려움
				2. 평균 기반의 복원으로 인해 흐릿한 이미지
	2. 암시적 밀도 추정
		1. 적대적 생성 신경망(GAN): 데이터 분포를 명시적으로 정의하지 않고 실제와 유사해보이는 샘플 생성에 집중
			1. 생성자: 랜덤 노이즈를 입력받아 실제와 유사한 데이터를 생성
			2. 판별자: 입력된 데이터가 진짜인지 생성자로 만들어진 가짜인지 판별
			3. 최소-최대 목적 함수: 제로섬 게임 수학적 표현
				1. 교대 학습
					1. 생성자와 판별자 교대로 업데이트
					2. 생성자 초기 기울기 소실 완화하기 위해 대체 목적 함수 사용
			- 구조: 생성자,판별자 2개의 모듈
				- 생성자: 랜덤 노이즈z를 입력으로 받아서 실제와 유사한 데이터를 생성하는 역할
					- 판별자가 구분하기 어렵도록 학습
				- 판별자: 생성되거나 실제인 데이터를 받아서 진짜인지 가짜(생성자가 만들어낸 것)인지 판별
					- 생성자가 생성한 데이터와 실제 데이터를 잘 구분하도록 학습
					- 생성자와 판별자가 적대적으로 학습을 하기 때문에 적대적 생성 신경망
			- 학습 과정
				- 랜덤 노이즈 입력을 생성자를 이용해서 가짜 이미지 만듦
				- 판별자는 가짜 이미지와 실제 이미지를 입력으로 받아서 분류 작업 수행
					- 틀림 -> 역전파 과정으로 학습(판별자(분류 잘하도록) -> 생성자(분류 못하도록))
					- 실제 이미지쪽으로도 역전파 과정 일어남
			1. 장점
				1. 다른 모델에 비해 선명하고 현실적인 데이터 생성
				2. 직관적이고 고수준 속성을 조작 가능
				   (남자,안경) - (남자,안경x) + (여자,안경x) = (여자,안경)
			2. 단점
				1. 훈련 불안정성: 생성자와 판별자의 손실이 진동
				2. 모드 붕괴: 생성자가 소수의 패턴만 생성



---
### **1. 생성 모델 개요 (Generative vs. Discriminative)**


- **판별 모델 (Discriminative Models):**
    
    - **목표:** 데이터 $x$가 주어졌을 때 레이블 $y$를 예측 (결정 경계 학습).
        
    - **모델링:** 조건부 확률 $p(y|x)$1.
        
    - 예: 분류기(Classifier).
        
- **생성 모델 (Generative Models):**
    
    - **목표:** 데이터의 분포 자체를 학습하여 새로운 샘플을 생성하거나 데이터를 재구성.
        
    - **모델링:** 결합 확률 분포 $p(x, y)$ 또는 데이터 분포 $p(x)$2.
        
    - **핵심:** 학습된 분포로부터 '새로운 데이터'를 만들어내는 것3.
        

---

### **2. 생성 모델의 분류 (Taxonomy)**

확률 밀도를 다루는 방식에 따라 크게 두 가지로 나뉩니다.

1. **명시적 밀도 추정 (Explicit Density):** 데이터의 확률 밀도 함수를 직접 정의하고 계산 (예: VAE, PixelRNN/CNN)4.
    
2. **암시적 밀도 추정 (Implicit Density):** 확률 밀도를 명시적으로 정의하지 않고, 샘플을 생성하는 능력만 학습 (예: GAN)5.
    

---

### **3. 주요 모델별 상세 정리**

#### **A. PixelRNN / PixelCNN (Autoregressive Models)**

- **개념:** 이미지의 픽셀을 순차적으로 생성하는 모델. 이전 픽셀 정보들을 조건($Condition$)으로 다음 픽셀을 예측합니다6.
    
    - 수식: $p(x)=\prod_{i=1}^{n}p(x_{i}|x_{1},...,x_{i-1})$7.
        
- **차이점:**
    
    - **PixelRNN:** RNN 구조 사용.
        
    - **PixelCNN:** 마스킹된 CNN(Masked CNN) 구조 사용8.
        
- **한계점:** 픽셀을 순차적으로 생성하므로 병렬 처리가 불가능하여 **생성 속도가 매우 느림**9.
    

#### **B. 오토인코더 (Autoencoder, AE)**

- **구조:** Encoder(압축) $\rightarrow$ Latent Space($z$) $\rightarrow$ Decoder(복원).
    
- **특징:** 입력 데이터를 저차원의 잠재 공간으로 압축하여 특징(Representation)을 학습하는 비지도 학습10101010.
    
- **한계:** 입력 데이터의 분포를 학습하는 것이 아니라 압축 및 복원에 초점이 맞춰져 있어, **새로운 샘플 생성 능력은 없음** (잠재 공간이 연속적이지 않음)11.
    

#### **C. 변분 오토인코더 (Variational Autoencoder, VAE)**

- **개념:** AE의 한계를 극복하기 위해, 잠재 공간($z$)을 고정된 벡터가 아닌 **확률 분포(정규 분포)**로 모델링12.
    
- **핵심 매커니즘:**
    
    1. **인코더:** 입력 $x$를 평균($\mu$)과 분산($\sigma$)으로 매핑.
        
    2. **샘플링:** 분포에서 잠재 변수 $z$를 샘플링.
        
    3. **디코더:** $z$를 다시 이미지로 복원.
        
- **필수 암기 개념:**
    
    - **Reparameterization Trick:** 샘플링 과정은 미분이 불가능하므로, $z=\mu+\sigma\odot\epsilon$ ($\epsilon \sim N(0,1)$) 형태로 변환하여 역전파(Backpropagation)가 가능하게 만듦13131313.
        
    - **손실 함수 (ELBO):**
        
        - $$Loss = \text{Reconstruction Loss} + \text{Regularization Loss (KL Divergence)}$$
        - 재구성 항: 원본 복원 능력 ($||x-\hat{x}||^2$).
            
        - 정규화 항: 잠재 공간을 정규 분포 $N(0,1)$와 유사하게 만듦15.
            
- **장단점:** 생성 속도가 빠르고 잠재 공간의 의미적 조작이 가능하나, 평균 기반 복원 특성상 **이미지가 흐릿(Blurry)함**16.
    

#### **D. 적대적 생성 신경망 (GAN)**

- **개념:** 생성자(Generator)와 판별자(Discriminator)가 서로 경쟁(Adversarial)하며 학습17.
    
- **구조:**
    
    - **Generator ($G$):** 랜덤 노이즈($z$)를 받아 진짜 같은 가짜 이미지를 생성.
        
    - **Discriminator ($D$):** 입력이 진짜($Real$)인지 가짜($Fake$)인지 판별.
        
- **목적 함수 (Minimax Game):**
    
    - $$min_{G} max_{D} V(D, G) = \mathbb{E}_{x\sim p_{data}}[\log D(x)] + \mathbb{E}_{z\sim p(z)}[\log(1-D(G(z)))]$$
        
        18.
        
    - $D$는 위 식을 최대화(Max)하려 하고, $G$는 최소화(Min)하려 함 (Zero-Sum Game)19.
        
- **장단점:** VAE보다 **선명하고 현실적인 이미지**를 생성하지만, **학습이 불안정**하고 **모드 붕괴(Mode Collapse, 소수의 패턴만 생성)** 현상이 발생할 수 있음20202020.
    

---

### **4. 시험 대비용 비교표 (Recap)**

|**특징**|**PixelRNN/CNN**|**Variational Autoencoder (VAE)**|**GAN**|
|---|---|---|---|
|**밀도 추정**|명시적 (Explicit)|명시적 (Explicit, 근사)|암시적 (Implicit)|
|**생성 방식**|순차적 생성 (Pixel-by-Pixel)|잠재 분포 샘플링|잠재 노이즈 변환|
|**장점**|정확한 확률 분포 모델링|안정적 학습, 빠른 생성, 의미적 조작|**고화질**, 선명한 결과물|
|**단점**|**매우 느린 속도**|**흐릿한(Blurry) 이미지**|**학습 불안정**, 모드 붕괴|
|**핵심 키워드**|Autoregressive, Masking|**ELBO, KL-Divergence, Reparameterization**|**Minimax Game, Adversarial**|

---

### **5. 교수님을 위한 제언 (Next Step)**

학생들이 특히 어려워하는 부분은 **VAE의 수식(ELBO 및 Reparameterization Trick의 필요성)**과 **GAN의 손실 함수(Minimax)의 의미**일 것입니다.

- **시험 출제 포인트 제안:**
    
    1. AE와 VAE의 결정적인 차이(잠재 공간의 연속성/생성 가능 여부) 서술.
        
    2. Reparameterization Trick이 왜 필요한지 이유 설명.
        
    3. GAN의 손실 함수 식을 주고 $D$와 $G$가 각각 어떤 항을 최적화하는지 해석.
        
    4. Mode Collapse 현상의 정의와 의미.
        

혹시 이 내용을 바탕으로 **"기말고사 대비 연습 문제(퀴즈)"**를 몇 가지 만들어 드릴까요?