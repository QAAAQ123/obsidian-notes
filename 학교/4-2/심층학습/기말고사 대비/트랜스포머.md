## 트랜스포머 과정

### 인코더 계층
1. 입력 임베딩
	1. 토큰화
	2. 임베딩
	3. 위치 인코딩
2. 인코더
	1. 다중 헤드 셀프 어텐션
	2. 잔차 연결
	3. 계층 정규화
	4. 피드 포워드
	5. 잔차 연결
	6. 계층 정규화
	7. 트랜스포머블록 반복
3. 인코더 출력

### 디코더 계층
1. 출력 임베딩
	1. 토큰화
	2. 임베딩
	3. 위치 인코딩
2. 디코더
	1. 마스크된 멀티헤드 어텐션
	2. 잔차 연결
	3. 계층 정규화
	4. 멀티 헤드 크로스 어텐션(인코더 출력 참조)
	5. 잔차 연결
	6. 계층 정규화
	7. 피드 포워드
	8. 잔차 연결
	9. 계층 정규화
	10. 트랜스포머 블록 반복
3. 출력 투영층
	1. Linear: 단어 사전 크기만큼의 차원으로 변환
	2. 소프트맥스

---

## 트랜스포머
- 장기의존성 및 병렬처리 한계 해결
- 인코더(분석) + 디코더(생성) 구조
## 입력 임베딩
- 토큰화
	- 실제 토큰과 정수 인덱스 매핑 정보를 저장하고 있는 집합인 어휘 사전을 이용해 토큰을 **정수 인덱스 형태**로 표현한다
- 임베딩
	- 정수 인덱스 값을 고차원 실수 벡터(유의미한 정보)로 변환 하는 것
- 위치 인코딩
	- 트랜스포머는 토큰의 순서를 인식할 수 없기 때문에 위치 정보를 추가하는 것
	- 위치 인코딩 함수를 지속 증가 함수로 사용했을때 단점
		- 학습시 입력 시퀀스의 길이가 길어지면 위치 함수의 차이가 커져서 임베딩 값이 커진다
		- 테스트시 입력 시퀀스의 길이가 학습 데이터보다 길면 테스트 데이터를 잘 해석하지 못한다
	- sin,cos인 주기 함수를 이용해 위치 인코딩함 -> 주기로 인한 중복 값 발생
		- 서로 다른 주기와 위상의 sin,cos 사용
		- 차원이 깊어질 수록 주기가 길어짐
		- sin은 짝수 차원,cos은 홀수 차원에서 사용
- 위치 인코딩,입력 임베딩 둘 다 d_model 차원을 가진다

## 인코딩
1. 다중 헤드 셀프 어텐션: 입력 시퀀스 내 토큰의 관계 학습
2. 잔차 연결: 정보 손실 방지,학습 안정화
	- 값 폭발 문제 완화
3. 계층 정규화: 특징 차원을 평균 0,분산1로 정규화
	- 배치 크기와 시퀀스 길이에 영향 받지 않음
4. 피드 포워드: 각 벡터를 독립적으로 비선형 변환 -> 토큰별로 MLP가 생성됨
	- 토큰 내부의 특징 차원들간 정보를 재조합
5. 잔차 연결
6. 계층 정규화
7. 1-6: 트랜스포머 블록
8. 트랜스포머 블록 반복 실행하여 더 깊은 정보 학습
9. 인코더 입력 관계 -> 비선형

## 출력 임베딩
1. 토큰화
2. 임베딩
3. 위치 인코딩

## 디코딩:  
1. 마스크된 멀티 헤드 어텐션: 내부 정보의 관계성 잘 파악하기 위해서
	1. 잔차 연결 + 계층 정규화
	2. 인코더의 출력인 context vector는 사용하지 않는다
2. 멀티 헤드 크로스 어텐션: **인코더 출력 벡터를 참고하여 디코더 입력 벡터와의 관계성 학습**
	1. Query: 디코더 입력/Key,Value: 인코더 출력 벡터
	2. 잔차 연결 + 계층 정규화
3. 피드 포워드
	1. 잔차 연결 + 계층 정규화
4. 1-3의 트랜스포머 블록 반복 학습하여 더 깊은 정보 학습
5. 디코더 입력 관계 -> 인코더 출력과 디코더 입력 관계 -> 비선형

## 출력 투영층
- Linear + softmax
- 디코더의 출력 벡터를 단어사전 크기로 변환
- Linear 층을 통해서 출력을 얻음
- 소프트맥스를 수행해서 예측 결과를 출력
- 출력 벡터의 차원: d_model
- 디코딩 출력을 사전크기(R^V)로 변환

### 실제 과정
- 다음에 어떤 단어를 가져올지 알 수 없기 때문에 순차적으로 생성하고 하나씩 결과를 출력한다
- End of sentence토큰이 생성될때까지 진행한다