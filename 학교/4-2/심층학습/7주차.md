- 코사인소멸의 하이퍼파라미터: T -> T는 epoch이기 때문에 자동으로 하이퍼파라미터 설정됨
- warmup: 초기에 w가 무작위이기 때문에 학습률이 크면 올바르지 않은 방향으로 흘러갈 가능성이 있음
- 안정화 후 초반에 높은 학습률 지정 가능 -> 학습 속도 빨라짐
- L2정규화
	- i = 각각의 weight
	- w의 값이 최종적으로 원의 형태가 됨 -> 원의 반지름이 L2 Distance가 됨
	- w중에 제일 큰 값부터 줄이도록 설계됨
	- 불필요한 가중치도 완전히 없애지 못하고 줄임
	- 필요/불필요 가중치를 잘 구분하지 못함 -> 해석력이 낮음
	- 안정성 
- L1정규화
	- 마름모(정사각형 형태)로 w가 제한됨
	- 람다=> L(w)와 R(w)의 비율 설정/손실 함수와 R(w)의 비율 얼마나 설정할지
	- 일부 가중치를 완전히 0으로 만들 수 있음 ->희소성 생김(희소성: 많은 값이 0을 가지게 되어서 가중치를 가진 값이 희소해지는 현상)
- 엘라스틱 넷
	- L1,L2동시에 추가 -> 희소성과 안정성 절충함
	- 방법 2개 있음
- 정규화 방법2: 드롭 아웃
	- 장점: 평균 가중치 크기 감소/특정 뉴런에 과도하게 의존하는 것 방지
	- m => 마스크
- 기울기 손실
	- 기울기가 0에 수렴하는 현상,딥러닝에서 가중치가.....
---
- 패널티 기반 정규화
	- 가중치 크기에 비례하는 페널티 항 추가
	- 어떤 distance를 사용하는가에 따라서 L2,L1,엘라스틱
- L2정규화
	- 가중치가 L2 distance를 넘을 수 없도록 한다(L2 norm)
	- 각각의 가중치의 제곱의 합으로 구성-> 원의 형태
	- 가중치의 크기가 제한되고, 가중치 크기 안에서 기존 손실 함수값이 가장 작은 것 선택->교점 최적화된 가중치
	- 특징
		- 2차함수형태의 가중치 -> 모든 가중치 점진적으로 감소시킴 -> 불필요한 가중치를 제거하지 않음 -> 해석력이 낮음
		- 모든 부분에서 미분 가능
		- 부드러운 경사로 안정적 수렴 가능
- L1정규화
	- 각각의 가중치에 절댓값의 합으로 패널티 항이 구성됨 -> 마름모의 형태
	- 각각의 가장 자리의 점의 길이는 모두 같다
	- 마름모 안에 있는 점 중에서 Loss가 가장 작은 점을 이다
	- 특징
		- 일차함수 형태 -> 일부 가중치를 0으로 수축시키는 특성 
		- 가중치가 감소 될때 Loss와 패널티의 비중을 비교하면 패널티항의 비중이 더 커지기 때문에 점점 작은 가중치를 가지게 되는 방향으로 학습을 진행한다.
		- 불필요한 가중치를 제거하기 때문에 해석력이 높다
		- 절댓값 형태로 되어있기 때문에 급격한 경사변화 -> 수렴이 불안정함
- 엘라스틱 넥
		- L1,L2의 장단점 절충한 정규화 방법
		- ~={red}L1의 희소성과 L2의 안정성=~ 절충
		- 서로의 장단점 보완해서 균형 잡힌 일반화
		- 하이퍼파라미터에 대한 높은 민감도를 가지고 있다
- 드롭 아웃
	- 학습 과정에서 일부 뉴런을 확률적으로 비활성화
	- 남은 뉴런만을 가지고 학습을 시킴
	- 장점
		- 전체 신경망의 성능이 특정 뉴런에 과도하게 의지하는 것을 방지
		- 어떤 뉴런이 저게 될지 모르기 때문에 모든 뉴런을 골고루 사용하여 학습을 함
	- 진행 과정
		- 배치마다 각 계층의 뉴런을 일정 확률 p로 비활성화
		- 특정한 뉴런을 비활성화 하기 위해서 마스크 설정
			- 마스크:확률p에 대해 1,1-p에 대해 0
		- 마스크: 0과 1의 조합으로 행렬 구성
		- 출력값과 마스크를 곱하면 0이되는 부분의 뉴런만 사라짐
	- 학습 단계: 뉴런에 마스크를 곱해서 학습
	- 추론 단계(test): 확률적으로 뉴런을 비활성화하면 전체 포텐셜을 이용할 수 없게됨.따라서 test시에는 모든 뉴런 사용->출력값의 스케일이 달라지는 문제 발생
	- 전체의 50%만 썼을때보다 100%를 쓰면 출력값이 2배가 되는 문제 발생-> 이 부분을 방지하기 위해서 p라는 확률을 h(출력값)에 일괄적으로 곱함 
	  ex)출력값에 0.5를 곱해서 테스트 단계의 스케일을 추론 단계의 스케일과 맞춰줌
	- 학습단계에서 드롭아웃을 통해서 50% 확률로 학습함 -> 추론 단계에서는 모든 뉴런을 다 사용하기 때문에 스케일을 맞춰주기 위해서 모든 출려값에 0.5를 곱해줌
	- 장단점
		- 과적합 방지,일반화 성능 향상
		- 앙상블 효과: 동일한 문제를 해결하는 다양한 독립적인 방법을 구성해서 출력 값을 종합해서 활용해서 더욱 안정적인 성능을 얻는 기법
		- 단점
			- 비활성화 시켜야 하기 때문에 학습 속도 저하
			- 하이퍼파라미터 튜닝 추가적으로 요구됨
			- 작은 네트워크에서 사용시 너무 뉴런수가 적어질수 있어서 과소적합문제가 발생 할 수 있음
- 기울기 소실
	- 역전파 과정에서 손실 함수의 기울기가 점점 작아져서 0에 가까워지는 현상
	- 깊은 신경망에서 주로 발생함
	- 입력층 근처에서 가중치가 업데이트 되지 않는 문제 발생(출력부분 부터 역전파로 업데이트 하기 때문에) -> 학습 정체
	- 원인
		- 활성화 함수의 미분값: 미분값이 0이 되면, 그 이후에 **모든 가중치가 0**이 된다.
		- 연속적인 곱셈: 1보다 작은 값이 계속 곱해지면 계속 작아져서 **0에 준하는 값으로 가중치가 설정** 되는 현상
	- 기울기 폭발: 손실함수의 기울기가 너무 커져서 무한대로 가서 학습이 불안정해지는 것
- 활성화 함수와 기울기 소실1-활성함수의 미분값 문제
	- 시그모이드(활성화 함수중 하나) 특징
		- 입력값이 매우 작거나 클때, 기울기 값이 0에 수렴한다 -> 기울기 소실 문제가 발생한다.
		- 0에서 1사이의 출력값을 가진다 
			- 장점: 출력값을 확률 형태로 나타낼 수 있다
			- 단점: 오차 신호에 의해서 부호가 결정된다.(활성화함수의 출력값이 부호에 영향을 미치지 않는다)->모든 gradient가 항상 같은 부호이다 -> 최적의 경로로 가지 못하고 지그재그로 학습해야 하는 문제가 있다 -> 학습이 불안정해지고 학습이 느려진다
	- 시그모이드 단점
		- 기울기 소실 문제
		- 지그재그 학습
- 활성화 함수와 기울기 소실2-활성화함수의 미분값 문제
	- 시그모이드와 유사한 모양,but 출력값이 -1에서 1사이의 값을 가지고 있다. -> 시그모이드의 지그재그 문제를 완화 
	- 하지만 입력값이 너무 작거나 클때 그레이디언트가 0에 수렴하는 문제 발생 -> 기울기 소실 문제
- 활성화 함수와 기울기 소실3-활성함수의 미분값 문제
	- ReLU
		- 양수인 경우에는 입력값을 그대로 출력값으로 사용,음수인 경우 모두 0의 출력값 -> 기울기가 0으로 수렴되는 문제 발생하지 않음 -> 기울기 소실 문제 발생하지 않음
		- 입력값이 음수일때는 출력값이 모두 0이 됨 -> 죽은 뉴런 문제
	- Leaky ReLU
		- 죽은 뉴런 문제 해결하기 위해 고안
		- 입력이 음수일때 출력값을 매우 작은 기울기로 설정
		- 음수값이 커지면 해당 값을 많이 학습에 반영하게 되는 단점
	- ELU
		- 마이너스 값에서 특정한 작은 값으로 수렴하도록
		- 마이너스 값이 커져도(노이즈가 커져도) 그래디언트를 0으로 만들어 주기 때문에 더 안정적으로 학습 할 수 있다.
- 가중치 초기화
	- 신경망의 가중치 초기 값 설정
	- 잘못 초기화 -> 기울기 소실 문제 or 폭주 문제 초래
	- 적절한 초기화 -> 빠른 수렴,높은 일반화 성능
	- 일반적으로 특정 확률 분포를 따르는 난수로 초기화 -> 잘못된 초기화가 수행될 가능성이 높아짐
- 잘못된 가중치 초기화
	- 한가지 값으로 초기화
		- Zero Initialization: 모든 가중치 0으로 초기화
		- Constant Initialiazation: 모든 가중치가 똑같은 값으로 초기화 -> 업데이트되는 값도 모두 동일해짐 -> 한 계층에서 모든 뉴런이 동일한 값을 가짐 -> 패턴 파악에 실패함
		- 대칭성 파괴 실패: 동일한 계층의 모든 뉴런이 항상 같은 값으로 업데이트 되는 현상
	- Large/Small Random initializaiton: 가중치가 너무 크거나 작은 값
	  시그모이드 함수를 예를 들면 그래디언트가 0이 됨 -> 기울기 소실 문제 발생 가능
- 기울기 소실 방지를 위한 다양한 초기화 방법1-대칭형 가중치 초기화
	- 자비에 초기화-왜 분산을 일정하게 유지해야 하는지
		- 출력과 기울기의 **분산이 일정하게 유지**
		- 입력,출력 노드수에 반비례하도록 설정
		- 분산이 너무 크면 가중치의 절댓값이 커져서 기울기 소실 문제 발생
		- 분산이 너무 작으면 가중치의 절댓갓이 작아져서 0주변의 값이 대부분이 되서 출력값이 0에 근사하게 됨 -> 학습이 올바르게 진행되지 않음
		- 균등 분포,정규 분포를 따르게 하는 2가지 방법이 있음 -> 상황에 적합한 방법 선택
	- 자비에 초기화-왜 입출력 노드수로 분산을 결정하는지
		- 순전파
			- 각 계층의 출력의 분산이 일정하게 유지되는 것이 목표
			- l계층에서의 분산: (이전 입력값 x 해당 가중치)의 분산
			- 출력,입력 분산의 유지가 목표 -> l계층에서의 출력 분산과 이전 입력값의 분산이 같아야 함 
			- 결론: 1/입력 노드수=l계층의 분산
		- 역전파
			- 기울기의 분산을 일정하게 유지하는 것이 목표
			- 순전파와 비슷한 논리
			- 결론: 1/출력 노드수=l계층의 분산
		- 역전파와 순전파의 결론을 절충해서 2/in+out으로 결정
	- 자비에 초기화-결론
		- 계층의 출력과 기울기의 분산이 일정하게 유지
		- 기울기 폭주,소실 방지
		- 빠르고 안정적인 수렴
		- 대칭형(시그모이드,Tanh등)에 적합
		- ReLU와 같은 비대칭형 활성화 함수에 사용하면 문제 발생
			- 음수모두 0이기 때문에 분산을 일정하게 유지했음에도 음수부분에의 해서 분산이 절반으로 감소하는 문제 발생 -> 비대칭형에는 다른 초기화 방법 사용해야 함
- 기울기 소실 방지를 위한 다양한 초기화 방법2-비대칭형 가중치 초기화
	- He초기화
		- 츨력의 분산이 일정하게 유지되도록
		- 입력노드의 수만 고려함
		- 그레이언트가 일정하기 때문에 출력의 분산만 일정하게 유지하면 됨 
		- 출력의 분산만 고려하면 되기 때문에 입력 노드에 반비례하게 됨
		- 균등or정규 분포
	- He초기화 유도
		- 목표: 출력의 분산이 일정하게 유지
		- 출력이 l계층을 거치기 이전과 이후에 똑같게 하는것을 목표로 한다.
		- var(x_l) = 1/2Var(h_l)이유: 음수는 모두 0이기 때문에 분산을 절반만 가지게 된다.(출력이 입력의 절반이 된다.)
- 배치 정규화
	- 정규화 + 기울기 소실 문제 같이 해결
	- 배치 정규화: 입력데이터의 평균과 분산을 미니배치 단위로 정규화 하는 방법
	- 동작 과정
		- 정규화: 입력 미니배치의 평균을 0으로,분산을 1로 조정
		- 스케일 및 이동: 학습 가능한 파라미터인 스케일과 이동을 적용-각각의 입력데이터의 특성에 맞도록 분포를 약간 변형:y값을 다음 입력 데이터로 활용
	- 각각의 미니배치에서 **활성화 함수 이전에 적용**됨->입력값이 일정해짐 -> 기울기 소실 문제 방지
	- 배치 정규화는 각 채널(독립적 특징 표현 공간)마다 정규화
	- 각각의 뉴런들이 하나의 채널에 해당된다고 생각하면된다.
- 이미지 데이터의 채널 3개: R,G,B
	- 정규화가 채널마다 독립적으로 적용
	- N: 배치 단위의 데이터 개수
	- C: 채널 개수
	- H,W: 이미지의 높이와 폭
	- 첫번째 그림이 배치 정규화-한 미니배치의 모든 데이터(H,W)에 대해서 정규화 적용한다.
	- Layer Norm: 하나의 계층을 단위로해서 정규화 하는 방법
		- 모든 채널과 하나의 입력 데이터에 대해서 정규화를 진행하지만,각각의 데이터별로 다르게 정규화 진행-참고
	- Instance Norm: 하나의 인스턴스에 대해서만 정규화를 진행-참고
- 배치 정규화 효과
	- 학습 속도와 안정성 향상-내부 공변량 변화(입력 데이터 분포가 계속 변하는 현상) 해결하기 때문에
		- 변동량이 누적되기 때문에 계속 증폭되면 출력에서 출력이 이상하게 나옴
	- 기울기 소실 문제 완화
		- 정규화 해서 포화구간으로 진입하는 것 방지
		- 출력 분포 일정하게 해서 각 층의 기울기 커지거나 작아지는거 방지
- 추론 단계에서 배치 정규화 적용
	- 안정된 출력을 위해서 현재 배치의 평균,분산 계산안함
	- 대신에 학습때 저장된 전역 평균과 전역 분산 사용
	- 학습된 파라미터인 스케일과 이동을 사용
	

