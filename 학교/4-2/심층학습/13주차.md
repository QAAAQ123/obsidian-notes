 RNN: 순차 데이터 처리/입력 시퀀스 길이가 길어질 경우 장기의존성 문제,연산속도 느림
 Convolution: 병렬적 처리(연산) => 연산 효율성 증가/국소 관계성만 파악,멀리 떨어져 있는 관계의 파악이 어려움
 Self-attention: 장기 의존성 해결,병렬 연산을 통한 효율성 증가
 - 트랜스포머 
	- 장기 의존성,병렬처리 한계 해결
	- 인코더(분석) + 디코더(생성)구조
- 인코더: 입력 문장 받음(전처리 과정,입력 임베딩)
1. 토큰화: 토큰 단위로 분리,토큰을 정수 인덱스로 표현
	1. 임베딩: 정수 인덱스로 표현된 토큰을 모델이 이해 가능한 의미기반의 고차원 실수 벡터로 변환
	2. 임베딩 값이 같은 수록 유사도가 높음
2. 임베딩 전에 위치 인코딩을 해서 순서 정보 부여
	1. 위치인코딩 고려사항: 5가지
	2. 함수 이용해서 순서/시간 정보 부여
		1. 한계
			1. 함수가 우상향하면 입력 길이가 길기 때문에 전체 값이 커짐(절댓값 증가)
			2. 학습과 테스트 데이터 길이가 다르면 모델이 해석하기 어려움
		2. 결론
			1. 유한한 범위를 가지는 함수 이용해야 한다. -> 삼각함수 이용 -> 값이 고유하지 않음,순환적 패턴 -> 삼각함수 여러개 이용해서 고유성 해결
			2. 여러개의 차원에서 sin,cos의 주기와 위상을 다르게 하여 해결
			3. 홀수 차원: cos,짝수 차원: sin
			4. 차원이 깊어질수록 주기가 길어짐
			5. 절대적,상대적 위치 반영이 쉬움
3. 위치 인코딩 + 입력 임베딩: 둘다 d_model 차원을 가짐
4. 어텐션 연산: 입력 해석
	1. 4개 부분으로 구성
	2. 멀티 헤드 어텐션
	3. 잔차 연결: ResNet의 잔차와 비슷하게, 정보 손실 방지,안정화
	4. 계층 정규화: 토큰 단위로 정규화 수행,유동성 있게 토큰 정규화
	5. 피드 포워드: 독립적으로 비선형 변환 => 토큰 별로 MLP 생성됨
5. 인코딩 결과 벡터: y1,2,3,4 => 이걸 디코딩
6. 입력과 출력 차원이 같기 때문에 트랜스포머 블록 계속 반복 가능


--- 
수요일
복습
1. 트랜스포머
	1. 구글 딥마인드에서 발표한 인공 신경망 구조
	2. 장기 의존성 문제 및 병렬 처리의 한계를 어텐션을 통해 해결
2. 인코더와 디코더로 구성

### 인코더
1. 토큰화: 토큰을 정수 인덱스 형태로 표현하고, 실제 토큰과 정수 인덱스 매핑 정보를 저장하고 있는 집합인 어휘 사전에 저장해 놓는다
2. 임베딩: 정수 인덱스 값을 고차원 실수 벡터로 변환 하는 것 
	1. 고차원 실수 벡터:유의미한 정보
3. 위치 인코딩: 요소 합이나 곱을 이용해서 위치 정보를 임베딩 벡터에 추가 
	1. 지속 증가 함수를 사용했을 때 단점: 입력 시퀀스의 길이가 길면 값 차이가 커진다,테스트시 입력 길이가 학습보다 길면 테스트 데이터를 잘 해석하지 못한다,
	2. 사인,코사인의 주기함수를 이용해서 위치 인코딩함
		1. 단점: 중복된 값 발생(주기성 문제)
	3. 그래서 여러개의 sin,cos함수 사용해서 절대적 위치 표현
		1. 각 차원마다 서로 다른 주기와 위상의 sin,cos사용
		2. 차원이 깊어질 수록 주기가 길어짐
4. 인코더 계층
	1. Multi-head Attention(다중 헤드 어텐션): 입력 시퀀스 내 토큰의 다양한 관계를 학습
		1. 셀프 어텐션 형태
	2. Residual Connection(잔차 연결): 정보 손실을 방지하고 학습을 안정화-입력값을 다중 셀프 어텐션 결과에 더해줌
	3. Layer Normalization(계층 정규화): 각 토큰의 특징 차원을 정규화-값 폭발문제 완화
	4. Feed Forward: 각 토큰의 표현 벡터를 독립적으로 비선형 변환-벡터별로 다른 MLP가짐
5. 잔차 연결
6. 계층 정규화
7. 최종 output -> 디코더로 전달함
4를 트랜스포머 블록이라고 명명함
일반적으로 트랜스포머 블록을 여러번 반복하여 더 깊은 정보 학습함(반복 학습 할 수 있는 이유: 입력,출력 차원이 같기 때문에)


## 디코더 계층
- 인코더가 추출한 입력 문장 표현을 참조하여 다음 토큰을 순차적으로 생성
- 3개의 서브 계층
	- Masked Multi-Head Attention,Cross-Attention,Feed Forward
- 인코딩을 거친 단어를 Tokenizer가 토큰화 하는 과정을 다시 거침(이때 독일어 어휘사전이 필요함)
1. 토큰화
2. 임베딩
3. 위치 인코딩(1-3의 전처리 과정은 인코더와 동일)
4. 마스크된 멀티헤드 어텐션
	1. 연산의 목적: 내부의 정보를 더 잘 파악하기 위해서(관계성만 파악)
	2. 따라서 인코더에서 해석된 **context벡터는 전혀 사용하지 않는다**
	3. 입력 시퀀스의 해석을 시간적,순서적 제약 내에서 수행(미래의 정보를 참조하지 않고 해석)
	4. 실제: 유사도 구함->마스크 행렬 만듦(0과 -무한대)->유사도 + 마스크 행렬 -> 어텐션 연산 -> 마스크된 어텐션 나옴 -> 마스크된 어텐션과 실제 값의 곱셉 -> 최종 출력
5. 크로스 어텐션: 인코더 출력 벡터와 디코더 입력 벡터와의 관계성 학습
	1. Query: 디코더 입력 벡터
	2. key,value: 인코더 출력 벡터
	3. 크로스 어텐션 연산 
6. Feed Forward: 인코더와 동일하게 진행
7. 4-6과정마다 잔차 연결과 계층 정규화를 진행함
8. 4-6블록을 합쳐서 트랜스포머 블록이라고 함
- N번 트랜스포머 블록을 반복하여 더 깊은 정보를 학습함

- 트랜스포머를 반복하여 나온 출력함
1. 출력 투영 층: 디코딩 출력을 사전크기(R^V)로 변환
	1. 출력 벡터의 차원: d_model
	2. Linear 층을 통해서 출력을 얻는다
	3. 소프트맥스를 수행해서 V차원중에 어떤 단어를 예측하는지 결과값을 출력

## 전체 과정
1. 입력
2. 임베딩
3. 위치 임베딩
4. 인코더
5. 인코딩 결과
6. 디코딩 과정에서 인코딩 결과 활용해서 K,V구함
7. 크로스 어텐션에서 인코딩 결과를 디코더에 전달
8. Linear와 Softmax를 사용해서 벡터 형태에서 실제 단어로 변환한다

- 실제 과정에서는 다음에 어떤 단어를 가져올 수 없기 때문에 순차적으로 생성하고 하나씩 결과를 출력한다
- EOS 토큰이 생성될때까지 진행한다 

## 트랜스포머 기반 모델
- 인코더: 특징 추출에 특화
	- BERT(버트)계열의 모델에서 많이 사용
- 디코더: 생성에 특화
	- GPT,LLaMA등