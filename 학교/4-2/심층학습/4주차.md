모델 구조
17. 신경망 모델 성능 결정 요인
	1. 모델 복잡도: 층 개수(모델의 깊이) x 노드 개수(모델의 폭)
	2. 활성화 함수 
	3. 과적합,과소적합 문제
18. 과적합 VS 과소적합
	1. 과하게 학습,데이터의 잡음까지 학습
	2. 학습을 충분히 학습하지 못함 -> 모델 단순할 때 자주 발생
	3. 적절한 모델
		1. 과소,적절,과적합
19. 분류 문제
	1. 과소
	2. 적절한 모델(큰 흐름에서 맞음)
	3. 과적합
20. 구조 결정 방법
	1. 과소,과적 문제 발생 안하게
	2. 과적합 => 모델 복잡도 감소시켜야 함
	3. 과소적합 => 모델 복잡도 증가 시켜야 함
21. ㄷ
22. ㄷ
23. 파라미터
	1. 파라미터: 모델 스스로가 수정하는 변수
	2. 하이퍼파라미터: 사용자가 정하는 변수-단일 학습에서는 변하지 않지만, 전체 학습 과정에서는 변한다
24. 하이퍼파라미터: 학습률,훈련 횟수,배치 크기
25. 하이퍼파라미터 학습률
	1. 학습률 적음: 학습 속도 느려짐/과소적합한 문제 발생 가능성
	2. 학습률 큼: 학습 속도 빨라짐
	3. 학습률 잘못 설정 했을 때 어떤 문제가 생기는지
		1. 최소 손실점: Global minima
		2. 주변에서 손실이 '상대적으로' 손실이 적은 점: local minima
		3. 로컬 미니마를 잘 피해서 global minima에 도달해야 한다
		4. 학습률 크면 global minima에 가지 못할 위험 존재(한번에 이동을 많이 하기 때문에)
		5. 학습률 작으면 local minima에서 빠져나가지 못할 위험 존재(조금씩 이동하기 때문에)
	4. 안장점: A<->B는 안장점이 손실 최소점,D<->C에서는 맨 아래가 최소 손실점
26. 성능 결정 요인
	1. 배치 크기: 훈련 데이터 사용 개수(각 배치에서 사용한 데이터는 다시 사용 불가)
	   예시)100개를 25개씩 4번 -> ~={red}전체 데이터 학습을 1번 끝낸 상태(4배치) 면 1epoch이다,4epoch가 아니다. =~
	2. 훈련 횟수
27. 데이터 개수를 배치 크기만큼 나눠서 훈련 횟수만큼 데이터 돌림
28. 배치 쓰면 전체 크기를 모두 반영 할 수 없음
29. 배치 크기 증가-> 전체 크기를 더 잘 나타냄 -> 성능 증가,하지만
30. 배치 크기 증가-> 더많은 GPU 메모리(물리적,현실적 한계가 있음)
31. 각 배치는 전체 분포를 반영 할 수 없기 때문에 배치마다 노이즈가 생긴다
	1. **~={red}노이즈는 안정적인 학습을 방해하지만,무작위성을 추가하기 때문에 로컬 미니마에서 빠져나오도록 동력을 만들어주기도 한다=~**
32. ㄷ
33. ㄷ
34. 최적화: 특정 목표(손실함수결과 최소화)를 달성하기 위한 과정
	1. 손실함수: 오차를 정의하는 함수
	2. 최적화: 의미 명확하게
	3. 특정 목표: 손실 함수 최소화 -> 손실 해가 가장 작은 것(~={red}딥러닝에서만=~) 
	4. ERM: 평균 손실 최소화
	5. 위험과 손실은 비슷하가
	6. 손실에 대한 기댓값
	7. 경험적 위험 최소화-> 파라미터 최적화
---
### 동영상
퀴즈
1. 퍼셉트론 설명: 단층 퍼셉트론으로는 비선형 문제를 해결할 수 없다
2. 2번 문제
	1. 은닉층은 층 개수와 노드 개수를 자유롭게 설정가능하지만, 입력층은 자유롭게 설정 불가
	2. 다층 퍼셉트론은 비선형 함수를 함께 사용하여 '일부의' 비선형 문제를 해결 할 수 있다
3. 비선형 문제인 XOR을 선형 결정 경계로 풀 수 없다
4. 다층과 단층 퍼셉트론의 결과가 똑같기 때문에, 활성화 함수가 필요하다
5. 순전파: 입력값으로 부터 출력값 얻는 과정
	1. 역전파: 출력값으로 부터 오차를 구해서 가중치를 업데이트하는 과정
6. w <- w - dL/dW
7. 기여도 할당 문제를 해결하기 위해서
8. 1x0.5 + 2x1 + 2 = 2.5 + 2 = 4.5 > 0 따라서 출력은 1

### Recap
1. 신경망 모델의 구조
	1. 복잡도 결정 요인: 층 개수,노드 개수
	2. 활성화 함수: 시그모이드 함수만 사용
	3. 활성화 함수에 따라 성능이 달라짐
	4. 과적합,과소적합 문제 발생 가능성
2. 
	1. 과적합: 지나치게 복잡(훈련 데이터의 잡음 까지 학습)-다른 데이터에 대한 성능이 낮음-오른쪽 그래프
	2. 과소적합: 지나치게 단순(충분히 학습하지 못한 상태)-훈련 성능과 다른 데이터에 대한 성능 모두 낮음-왼쪽 그래프
	3. 적절한 모델-가운데 그래프
3. 모델 복잡도: 층 x 노드 개수
	1. 작은 모델로 시작 -> 노드 수 증가(모델의 표현력 자체 증가)-> 층 수 증가(비선형 함수 증가)
	2. 층 수가 많은 모델은 비선형 함수를 곱셈하는 과정이 늘어나면서 과적합 및 기울기 소실 문제 발생 가능성
4. 하이퍼파라미터
	1. 사용자가 고정한 외부 변수
	2. 종류
		1. 학습률: 파라미터를 업데이트하는 속도
			1. 학습률이 작으면:이동속도가 느려짐 + 로컬미니마에 빠지기 쉬움
			2. 학습률이 크면: 이동속도가 빨라서 진동하는 문제 + 수렴이 잘 되지 않는 현상 발생 가능성   
		2. 훈련 횟수
			1. 횟수 증가: 학습 데이터를 더 잘 학습,but 너무 크면 과적합 문제 발생 가능성
		3. 배치 크기
			1. 1개씩 100번/10개씩 10번/100개씩 1번 등
			2. 한 묶음: 배치
			3. 배치 크기 크면: 모델을 조금더 정확하게 반영 가능
			4. 크기 작으면: 전체 분포를 정확히 반영 불가 + 노이즈 발생
			5. 노이즈: 안정적인 학습 방해,무작위성 추가하여 로컬 미니마에서 탈출하는 동력 만들어주기도 함
5. 최적화: 정해진 제약조건 안에서 특정 목표를 달성하기 위해 최적해를 찾는 과정
	1. 정해진 제약 조건: 하이퍼파라미터,모델의 구조,가중치와 편향 등...
	2. 특정 목표: 손실 함수 최소화(딥러닝에서는)
	3. 경험적 위험 최소화, 우도 추정 등 목표에 따라서 다른 함수 설정
6. ERM: 경험적 위험 최소화-훈련 데이터의 평균 손실 최소화
	1. 위험: 손실을 의미/상상할 수 있는 모든 데이터의 집합으로부터 얻을 수 있는 손실을 의미
	2. P_data는 가상의 존재할 수 있는 모든 데이터의 집합(이 데이터는 학습 불가)
	3. P_data를 근사하여 사용 학습 데이터로 근사하여 위험을 구하는 방법: 경험적 위험(Cost function)-n개의 실제 데이터를 사용하여 위험을 구함
	4. 경험적 위험 최소화(ERM): 세타만 변경 할 수 있기 때문에 최소 값이 되는 세타를 찾는 과정이 경험적 위험 최소화이다
7. 용어 정리
	1. 다이어그램과 표로 정리되어있음
	2. Cost function과 Risk는 loss function기반
	3. Cost function은 **실제 데이터로 손실 계산**
	4. Risk는 **전체 데이터를 가정**하고 손실 계산
8. 손실 함수
	1. 대표적인 함수: 평균 제곱 오차(MSE)
	2. 사과,바나나의 정답 클래스는 원합 인코딩: **~={red}각각의 차원이 하나의 클래스 나타냄=~**
	3. [1,0]에서 첫번째는 사과인지 아닌지,0은 바나나인지 아닌지를 나타냄
	4. K개의 데이터를 받아서 계산한 다음 K개로 나눠서 평균을 냄
	5. 사과의 오차: 0.05->예측대로임
	6. 바나나의 오차: 0.65->예측과 반대로임 
	7. 대표적인 함수2: 교차 엔트로피(CE)
	8. 정답과 예측 확률 분포의 차이를 이용한다
	9. 유사도 차이
	10. y_k: 정답 확률/y_hat_k: 예측 확률
	11. 마이너스가 붙어있음 이유: 정답과 예측 확률이 유사할때 값이 작아지도록 만들기 위해서(유사할 수록 값이 커지기 때문에)
	12. 이 상황에서 정답 확률은 그대로 확률이 된다.(합이 1이고 최대값이 1이하이기 때문에)
	13. z(로짓): 시그모이드 적용 이전 값/확률이 아님
	14. o(출력 값): 확률이 아님/더했을때 1이 아니기 때문에
	15. 로짓은 Wx+b이기 때문에 확률이 안된다
	16. z(로짓) = Wx+b
	17. 출력값(z에 시그모이드를 한 값)도 확률이 아니다. 모두 합했을때 1이 아니기 때문에
	18. **~={red}출력값을 확률분포로 변환해야 한다=~**
	19. 로짓을 CE로 할때 시그모이드 적용하지 않는다
	20. 로짓을 확률분포로 변환해서 확률값이 되도록 해준다
	21. ~={red}**출력값을 확률로 변하게 만들어주는 함수가 소프트맥스 함수**=~이다
	22. 소프트맥스 함수의 분자e^Z_k: 지수값이다 따라서 항상 0보다 크다
	23. 분모: 지수값의 합이기 때문에 항상 0보다 크다
	24. 확률의 각 값은 모두 0<= x <= 1이여야 한다는 조건: 분모는 분자의 모든 값의 합을 더한 값이기 때문에 항상 분자보다 크다 -> y_k는 항상 0보다 크거나 같고 1보다 작거나 같다
	25. 각 값을 모두 더했을때의 값은 항상 1이여야 한다는 조건: 분모와 분자 모두 e^Z_k를 모두 더한 값이기 때문에 모두 더한 값이 항상 1이다
	26. 소프트맥스 함수에 굳이 지수함수를 사용하는 이유
		1. 항상 양수이기 위해
		2. 값의 차이를 강조하기 위해
	27. 로짓은 정답과 정답이 아닌 클래스 모두를 예측해야 한다(정답이 아닌 클래스에서는 항상0이 나오기 때문에 할 수있는 생각-확률 분포를 계산해하기 때문에 모두 잘 예측 해야 한다)
	28. Log 쓰는 이유
		1. 오답에 강한 패널티 부여: 오답에 가까워질 수록 무한대에 가까워지고, 정답에 가까울 수록 1에 가까워지기 때문에(오답이 클수록 패널티가 기하급수적으로 거친다)
		2. 미분값 단순화-y_hat_k는 소프트맥스 함수를 이용하여 값이 나오기 때문에 복잡하다. 이걸 로그를 쓰게되면 단순해진다
	29. 학습 신호=기울기
	30. 분류문제: 데이터를 몇가지 카테고리로 나눔
	31. 포화영역: 예측과 실제의 차이가 큰 경우
	32. 회귀문제: 데이터의 경향성을 파악
	33. MSD와 CD 아래 3개가 중요
		1. 학습 특성: 정답이 1일때 0이나 0.4같은 정답과 비슷하지 않은 예측 값(포화영역)이 나오면 학습신호(기울기)가 소실 된다.